{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS445 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of hand-drawn letters using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**by Sanchit Talavdekar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I have used Convolutional Neural Networks (CNN) to classify hand-drawn letters. Taking inspiration from the MNIST data set which only had 10 classes, I wanted to train my CNN model on a more challenging dataset which would have even more classes. The dataset that I found for my project is of hand-drawn lowercase letters. This is the Stanford OCR dataset where each row represents a lowercase letter and has the following values:\n",
    "1.\tid: each letter is assigned a unique integer id\n",
    "2.\tletter: a-z\n",
    "3.\tnext_id: id for next letter in the word, -1 if last letter\n",
    "4.\tword_id: each word is assigned a unique integer id (not used)\n",
    "5.\tposition: position of letter in the word (not used)\n",
    "6.\tfold: 0-9 -- cross-validation fold\n",
    "7.\tp_i_j: 0/1 -- value of pixel in row i, column j\n",
    "\n",
    "The dataset is available here: http://ai.stanford.edu/~btaskar/ocr/\n",
    "\n",
    "There are 52152 letters to classify among 26 classes.\n",
    "\n",
    "Resources used from the class include:\n",
    "\n",
    "1) Function for generating stratified partitions of the dataset adapted from Lecture 10\n",
    "\n",
    "2) Code for CNN2D class for training the models adapted from assignment A5\n",
    "\n",
    "3) Function for constructing confusion matrix adapted from assignment A4\n",
    "\n",
    "Results showed that the best training model gave around 85% accuracy on the testing dataset.\n",
    "\n",
    "To understand the dataset more better let us take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv(\"letter.names\", delim_whitespace=True, header = None)\n",
    "data = pd.read_csv('letter.data', delim_whitespace=True, header=None, names = list(t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>letter</th>\n",
       "      <th>next_id</th>\n",
       "      <th>word_id</th>\n",
       "      <th>position</th>\n",
       "      <th>fold</th>\n",
       "      <th>p_0_0</th>\n",
       "      <th>p_0_1</th>\n",
       "      <th>p_0_2</th>\n",
       "      <th>p_0_3</th>\n",
       "      <th>...</th>\n",
       "      <th>p_14_6</th>\n",
       "      <th>p_14_7</th>\n",
       "      <th>p_15_0</th>\n",
       "      <th>p_15_1</th>\n",
       "      <th>p_15_2</th>\n",
       "      <th>p_15_3</th>\n",
       "      <th>p_15_4</th>\n",
       "      <th>p_15_5</th>\n",
       "      <th>p_15_6</th>\n",
       "      <th>p_15_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>o</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>m</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>m</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>n</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52147</th>\n",
       "      <td>52148</td>\n",
       "      <td>n</td>\n",
       "      <td>52149</td>\n",
       "      <td>6877</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52148</th>\n",
       "      <td>52149</td>\n",
       "      <td>t</td>\n",
       "      <td>52150</td>\n",
       "      <td>6877</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52149</th>\n",
       "      <td>52150</td>\n",
       "      <td>i</td>\n",
       "      <td>52151</td>\n",
       "      <td>6877</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52150</th>\n",
       "      <td>52151</td>\n",
       "      <td>a</td>\n",
       "      <td>52152</td>\n",
       "      <td>6877</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52151</th>\n",
       "      <td>52152</td>\n",
       "      <td>l</td>\n",
       "      <td>-1</td>\n",
       "      <td>6877</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52152 rows Ã— 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id letter  next_id  word_id  position  fold  p_0_0  p_0_1  p_0_2  \\\n",
       "0          1      o        2        1         1     0      0      0      0   \n",
       "1          2      m        3        1         2     0      0      0      0   \n",
       "2          3      m        4        1         3     0      0      0      0   \n",
       "3          4      a        5        1         4     0      0      0      0   \n",
       "4          5      n        6        1         5     0      0      0      0   \n",
       "...      ...    ...      ...      ...       ...   ...    ...    ...    ...   \n",
       "52147  52148      n    52149     6877        10     9      0      0      0   \n",
       "52148  52149      t    52150     6877        11     9      0      0      0   \n",
       "52149  52150      i    52151     6877        12     9      0      0      1   \n",
       "52150  52151      a    52152     6877        13     9      0      0      0   \n",
       "52151  52152      l       -1     6877        14     9      0      1      1   \n",
       "\n",
       "       p_0_3  ...  p_14_6  p_14_7  p_15_0  p_15_1  p_15_2  p_15_3  p_15_4  \\\n",
       "0          0  ...       0       0       0       0       0       0       0   \n",
       "1          0  ...       0       0       0       0       0       0       0   \n",
       "2          0  ...       0       0       0       0       0       0       0   \n",
       "3          0  ...       0       0       0       0       0       0       0   \n",
       "4          0  ...       0       0       0       0       0       0       0   \n",
       "...      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "52147      0  ...       0       0       0       0       0       0       0   \n",
       "52148      0  ...       0       0       0       0       0       0       0   \n",
       "52149      1  ...       0       0       0       0       0       0       0   \n",
       "52150      0  ...       0       0       0       0       0       0       0   \n",
       "52151      0  ...       0       0       0       0       0       0       0   \n",
       "\n",
       "       p_15_5  p_15_6  p_15_7  \n",
       "0           0       0       0  \n",
       "1           0       0       0  \n",
       "2           0       0       0  \n",
       "3           0       0       0  \n",
       "4           0       0       0  \n",
       "...       ...     ...     ...  \n",
       "52147       0       0       0  \n",
       "52148       0       0       0  \n",
       "52149       1       0       0  \n",
       "52150       0       0       0  \n",
       "52151       1       1       0  \n",
       "\n",
       "[52152 rows x 134 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the first row. Wait! We have pixel values in this row so let us consider those instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 1, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [1, 0, 0, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1 = data.iloc[0:1,6:].values\n",
    "# By observing column names we can conclude that there are 16 rows and 8 columns of an image\n",
    "image1 = image1.reshape(16, 8)\n",
    "image1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright. Let's use matplotlib to make an image display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAADnCAYAAAA+YylHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAACn0lEQVR4nO3dwYqDMBRA0XGY//9lu+6iEQKxce45WymIlweFh/E4z/OHht9v3wD3ETtE7BCxQ8QO+bu47q/68xyfLpjsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO+Tq0Lspx/Hx3LUtVI/dNtkhYoeIHSJ2iNghYoeIHSJ2iNghYoeIHSJ2yJJFyO7uXtTssngx2SFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CHbbL122QyNzG7LZn634nmY7BCxQ8QOETtE7BCxQ8QOETtE7BCxQ8QOETtE7JDh1mv3w+vuNruJmnmOs89+dI8mO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA5Z8vrPE17ludPM81ixhDLZIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idsiSQ+94t8tXlEx2iNghYoeIHSJ2iNghYoeIHSJ2iNghYoeIHSJ2SHLrtcsWamTF57JMdojYIWKHiB0idojYIWKHiB0idojYIWKHiB2yZBHyhEXDjBXLiTuZ7BCxQ8QOETtE7BCxQ8QOETtE7BCxQ8QOETtE7JDh1uvpWx7emewQsUPEDhE7ROwQsUPEDhE7ROwQsUPEDhE7ROwQsUPEDhE7ROwQsUPEDhE7ROwQsUPEDhE7ROwQsUPEDhE7ROwQsUPEDhE7ROwQsUPEDhE7ROwQsUPEDhE75OpTT//zm01RJjtE7BCxQ8QOETtE7JAXlHQm4pIShtMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(-image1, cmap='gray');\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like an 'o'. Let's look at the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['o']], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0:1,1:2].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is correct. Now let's take a look at first 100 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(100):\n",
    "    a = data.iloc[i:i+1,6:].values\n",
    "    b= data.iloc[:,1:2].values\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(-a.reshape(16, 8), cmap='gray')\n",
    "    plt.title(b[i,0])\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! So, the goal of this project is to train a CNN model to classify the letters in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in this dataset represents a lowercase letter. The pixel values in the row represent the image of the letter. This will be input to our model. Each row also has the label of that letter. This will be output of our model. We now make a input sample matrix containing the pixel values of the image and a output target matrix that will contain the class labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[0:,6:].values\n",
    "T = data.iloc[0:,1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52152, 128), (52152, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image of our dataset is of size 16x8. In order to prevent matrix multiplication errors, it would be more convenient if the height and width are the same values. So, we add four columns at the start and four columns at the end of an image to make the size of the image 16x16. The values of these additional columns will be 0. We need to make a new input matrix which will have the updated image pixel values. Let us call this new input matrix `newX`. It is derived by modifying the original input matrix `X`. The following is the code for constructing `newX`:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX = [] #Initialize newX as a blank list\n",
    "for orig in X: # Take each row of the original matrix X\n",
    "    blank = [0]*4\n",
    "    newImage = []\n",
    "    for row in orig.reshape(16, 8): #Take each row of the reshaped image of size 16*8\n",
    "        newImage.append(blank+list(row)+blank) #Add four elements of value 0 at the start and at the end \n",
    "    newX.append(np.array(newImage).flatten())  #Flatten the image and append to the list newX  \n",
    "    \n",
    "newX = np.array(newX) #Covert the list newX into a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the images in the new input sample `newX`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(-(newX[0].reshape(16, 16)), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!! Now we will not get any matrix multiplication errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52152, 256), (52152, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newX.shape,T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We partition our data into train, validation and test sets. Here, we use stratified partitioning to make each fold have approximately the same proportion of samples from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is adapted from Lecture 10. It is slightly modified to exclude the 'no validation set' condition.\n",
    "def generate_stratified_partitions(X, T, n_folds, shuffle=True):\n",
    "    \n",
    "    def rows_in_fold(folds, k):\n",
    "        all_rows = []\n",
    "        for c, rows in folds.items():\n",
    "            class_rows, starts, stops = rows\n",
    "            all_rows += class_rows[starts[k]:stops[k]].tolist()\n",
    "        return all_rows\n",
    "\n",
    "    def rows_in_folds(folds, ks):\n",
    "        all_rows = []\n",
    "        for k in ks:\n",
    "            all_rows += rows_in_fold(folds, k)\n",
    "        return all_rows\n",
    "\n",
    "    row_indices = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(row_indices)\n",
    "    folds = {}\n",
    "    classes = np.unique(T)\n",
    "    for c in classes:\n",
    "        class_indices = row_indices[np.where(T[row_indices, :] == c)[0]]\n",
    "        n_in_class = len(class_indices)\n",
    "        n_each = int(n_in_class / n_folds)\n",
    "        starts = np.arange(0, n_each * n_folds, n_each)\n",
    "        stops = starts + n_each\n",
    "        stops[-1] = n_in_class\n",
    "        folds[c] = [class_indices, starts, stops]\n",
    "\n",
    "    for test_fold in range(n_folds):\n",
    "        for validate_fold in range(n_folds):\n",
    "            if test_fold == validate_fold:\n",
    "                continue\n",
    "            train_folds = np.setdiff1d(range(n_folds), [test_fold, validate_fold])\n",
    "            rows = rows_in_fold(folds, test_fold)\n",
    "            Xtest = X[rows, :]\n",
    "            Ttest = T[rows, :]\n",
    "            rows = rows_in_fold(folds, validate_fold)\n",
    "            Xvalidate = X[rows, :]\n",
    "            Tvalidate = T[rows, :]\n",
    "            rows = rows_in_folds(folds, train_folds)\n",
    "            Xtrain = X[rows, :]\n",
    "            Ttrain = T[rows, :]\n",
    "            yield Xtrain, Ttrain, Xvalidate, Tvalidate, Xtest, Ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31316, 256), (31316, 1), (10418, 256), (10418, 1), (10418, 256), (10418, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain,  Ttrain,  Xval, Tval, Xtest,  Ttest = next(generate_stratified_partitions(newX, T, 5))\n",
    "Xtrain.shape, Ttrain.shape, Xval.shape, Tval.shape, Xtest.shape, Ttest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if our `generate_stratified_partitions` function worked correctly. The following code checks the sample distribution ratio in the train, validation and test sets for a randomly generated sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us consider the letter 'j'\n",
      "\n",
      "Ttrain has 115 samples\n",
      "Tval has 37 samples\n",
      "Ttest has 37 samples\n",
      "\n",
      "The sample distribution ratio is\n",
      "Ttrain : Tval : Ttest = 3.1 : 1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "uniques = np.unique(Ttrain)\n",
    "lisst = [Ttrain, Tval,  Ttest]\n",
    "lucky_letter = np.random.choice(uniques)\n",
    "print(f\"Let us consider the letter '{lucky_letter}'\\n\")\n",
    "a = []\n",
    "for l in lisst:\n",
    "    m = np.count_nonzero(l==lucky_letter)\n",
    "    a.append(m)\n",
    "print(f'Ttrain has {a[0]} samples\\nTval has {a[1]} samples\\nTtest has {a[2]} samples\\n')\n",
    "b = min(a)\n",
    "print(f'The sample distribution ratio is\\nTtrain : Tval : Ttest = {a[0]/b:.1f} : {a[1]/b:.1f} : {a[2]/b:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!! Our samples are distributed correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our 2D CNN class which we will be using for training our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is adapted from assignment A5\n",
    "class CNN2D(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, n_outputs,\n",
    "                 patch_size_per_conv_layer, stride_per_conv_layer, activation_function='tanh', device='cpu'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        n_conv_layers = len(n_hiddens_per_conv_layer)\n",
    "        if (len(patch_size_per_conv_layer) != n_conv_layers\n",
    "            or len(stride_per_conv_layer) != n_conv_layers):\n",
    "            raise Exception('The lengths of n_hiddens_per_conv_layer, patch_size_per_conv_layer, and stride_per_conv_layer must be equal.')\n",
    "\n",
    "        self.activation_function = torch.tanh if activation_function == 'tanh' else torch.relu\n",
    "\n",
    "        self.make_conv_and_fc_layers(n_inputs, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, n_outputs,\n",
    "                                     patch_size_per_conv_layer, stride_per_conv_layer)\n",
    "        \n",
    "        self.Xmeans = None\n",
    "        self.to(self.device)\n",
    "\n",
    "    def make_conv_and_fc_layers(self, n_inputs, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, n_outputs,\n",
    "                                patch_size_per_conv_layer, stride_per_conv_layer):\n",
    "                # Create all convolutional layers\n",
    "        # First argument to first Conv2d is number of channels for each pixel.\n",
    "        # Just 1 for our grayscale images.\n",
    "        n_in = 1\n",
    "        input_hw = int(np.sqrt(n_inputs))  # original input image height (=width because image assumed square)\n",
    "        self.conv_layers = torch.nn.ModuleList()\n",
    "        layeri = 0\n",
    "        for nh, patch_size, stride in zip(n_hiddens_per_conv_layer,\n",
    "                                          patch_size_per_conv_layer,\n",
    "                                          stride_per_conv_layer):\n",
    "            self.conv_layers.append(torch.nn.Conv2d(n_in, nh, kernel_size=patch_size, stride=stride))\n",
    "            conv_layer_output_hw = (input_hw - patch_size) // stride + 1\n",
    "            if conv_layer_output_hw <= 0:\n",
    "                raise Exception(f'''For conv layer {layeri}, input_hw of {input_hw} is less than patch_size {patch_size}.\n",
    "Try reducing the patch_size for this layer or for the previous layer.''')\n",
    "            input_hw = conv_layer_output_hw  # for next trip through this loop\n",
    "            n_in = nh\n",
    "            layeri += 1\n",
    "           \n",
    "        # Create all fully connected layers.  First must determine number of inputs to first\n",
    "        # fully-connected layer that results from flattening the images coming out of the last\n",
    "        # convolutional layer.\n",
    "        n_in = input_hw ** 2 * n_in\n",
    "        self.fc_layers = torch.nn.ModuleList()\n",
    "        for nh in n_hiddens_per_fc_layer:\n",
    "            self.fc_layers.append(torch.nn.Linear(n_in, nh))\n",
    "            n_in = nh\n",
    "        self.fc_layers.append(torch.nn.Linear(n_in, n_outputs))\n",
    "\n",
    "    def forward_all_outputs(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        Ys = [X]\n",
    "        for conv_layer in self.conv_layers:\n",
    "            Ys.append(self.activation_function(conv_layer(Ys[-1])))\n",
    "\n",
    "        flattened_input = Ys[-1].reshape(n_samples, -1)\n",
    "\n",
    "        for layeri, fc_layer in enumerate(self.fc_layers[:-1]):\n",
    "            if layeri == 0:\n",
    "                Ys.append(self.activation_function(fc_layer(flattened_input)))\n",
    "            else:\n",
    "                Ys.append(self.activation_function(fc_layer(Ys[-1])))\n",
    "\n",
    "        if len(self.fc_layers) == 1:\n",
    "            # only the output layer\n",
    "            Ys.append(self.fc_layers[-1](flattened_input))\n",
    "        else:\n",
    "            Ys.append(self.fc_layers[-1](Ys[-1]))\n",
    "\n",
    "        return Ys\n",
    "\n",
    "    def forward(self, X):\n",
    "        Ys = self.forward_all_outputs(X)\n",
    "        return Ys[-1]\n",
    "\n",
    "    def train(self, X, T, batch_size, n_epochs, learning_rate, method='sgd', verbose=True):\n",
    "        '''X and T must be numpy arrays'''\n",
    "\n",
    "        self.classes = np.unique(T)\n",
    "        T = np.arange(len(self.classes))[np.where(T.reshape(-1, 1) == self.classes)[1]]\n",
    "\n",
    "        # Set data matrices to torch.tensors\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        T = torch.from_numpy(T).long().to(self.device)  # required for classification in pytorch\n",
    "\n",
    "        # Setup standardization parameters\n",
    "        if self.Xmeans is None:\n",
    "            self.Xmeans = X.mean(axis=0)\n",
    "            self.Xstds = X.std(axis=0)\n",
    "            self.Xstds[self.Xstds == 0] = 1  # So we don't divide by zero when standardizing\n",
    "\n",
    "        # Standardize X\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "\n",
    "        X.requires_grad_(True)\n",
    "\n",
    "        if method == 'sgd':\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        CELoss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.error_trace = []\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            num_batches = X.shape[0] // batch_size\n",
    "            loss_sum = 0\n",
    "\n",
    "            for k in range(num_batches):\n",
    "                start = k * batch_size\n",
    "                end = (k + 1) * batch_size\n",
    "                X_batch = X[start:end, ...]\n",
    "                T_batch = T[start:end, ...]\n",
    "\n",
    "                Y = self.forward(X_batch)\n",
    "\n",
    "                loss = CELoss(Y, T_batch)\n",
    "                loss.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss_sum += loss\n",
    "\n",
    "            self.error_trace.append(loss_sum / num_batches)\n",
    "\n",
    "            if verbose and (epoch + 1) % (max(1, n_epochs // 10)) == 0:\n",
    "                print(f'{method}: Epoch {epoch + 1} Loss {self.error_trace[-1]:.3f}')\n",
    "\n",
    "        return self\n",
    "\n",
    "    def softmax(self, Y):\n",
    "        '''Apply to final layer weighted sum outputs'''\n",
    "        # Trick to avoid overflow\n",
    "        maxY = torch.max(Y, axis=1)[0].reshape((-1, 1))\n",
    "        expY = torch.exp(Y - maxY)\n",
    "        denom = torch.sum(expY, axis=1).reshape((-1, 1))\n",
    "        Y = expY / denom\n",
    "        return Y\n",
    "\n",
    "    def use(self, X):\n",
    "        # Set input matrix to torch.tensors\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        # Standardize X\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        # Calculate output of net for all samples in X\n",
    "        Y = self.forward(X)\n",
    "        # Convert output to class probabilities\n",
    "        probs = self.softmax(Y)\n",
    "        # For each sample pick highest probability and translate that to class labels\n",
    "        classes = self.classes[torch.argmax(probs, axis=1).cpu().numpy()].reshape(-1, 1)\n",
    "        return classes, probs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below makes our system to perform the pytorch computations on the GPU if the system has a cuda enabled GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to run on the GPU? (y or n): y\n",
      "Running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    y_or_n = input('Would you like to run on the GPU? (y or n): ')\n",
    "    if y_or_n == 'y' or y_or_n == 'yes':\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will require a function that tells us the accuracy of the trained model. So, lets write a code for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y,t):\n",
    "    perc_correct = 100 * np.mean(y == t)\n",
    "    return perc_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require a similar function for our experiments for calculating and printing the accuracy of training, testing and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printstuff(Xtrain,Ttrain,Xtest,Ttest,Xval,Tval):\n",
    "    \n",
    "    print(f'Training took {elapsed_time} seconds.')\n",
    "    Classes_train, _train = cnnet.use(Xtrain)\n",
    "    train_perc_correct = accuracy(Classes_train,Ttrain)\n",
    "    print(f'Train accuracy in percent correct: {train_perc_correct:.2f}')\n",
    "    \n",
    "    Classes_test, _test = cnnet.use(Xtest)\n",
    "    test_perc_correct = accuracy(Classes_test,Ttest)\n",
    "    print(f'Test accuracy in percent correct: {test_perc_correct:.2f}')\n",
    "    \n",
    "    Classes_val, _test = cnnet.use(Xval)\n",
    "    val_perc_correct = accuracy(Classes_val,Tval)\n",
    "    print(f'Test accuracy in percent correct: {test_perc_correct:.2f}')\n",
    "    print('\\n')\n",
    "    \n",
    "    return train_perc_correct,test_perc_correct,val_perc_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will require column names for the `pandas` dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ('Epochs','Batch size','Learning Rate','Hiddens per convolutional layer', 'Patch size per convolutional layer', 'Stride per convolutional layer', 'Hiddens per fully-connected layer', 'Train percent correct', 'Test percent correct','Validate percent correct', 'Training time (seconds)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also require confusion matrix to study our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is adapted from assignment A4\n",
    "def confusion_matrix(Y_classes, T):\n",
    "    class_names = np.unique(T)\n",
    "    table = []\n",
    "    for true_class in class_names:\n",
    "        row = []\n",
    "        for Y_class in class_names:\n",
    "            row.append(100 * np.mean(Y_classes[T == true_class] == Y_class))\n",
    "        table.append(row)\n",
    "    conf_matrix = pd.DataFrame(table, index=class_names, columns=class_names)\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape Xtest, Xval, Xtest because convolutional nets in pytorch require reshaping the input matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31316, 1, 16, 16),\n",
       " (31316, 1),\n",
       " (10418, 1, 16, 16),\n",
       " (10418, 1),\n",
       " (10418, 1, 16, 16),\n",
       " (10418, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain = Xtrain.reshape(-1, 1, 16, 16)\n",
    "Xval = Xval.reshape(-1, 1, 16, 16)\n",
    "Xtest = Xtest.reshape(-1, 1, 16, 16)\n",
    "\n",
    "Xtrain.shape, Ttrain.shape, Xval.shape, Tval.shape, Xtest.shape, Ttest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Now that we have defined all the functions and classes that we need, let us run a test to check if the model training works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam: Epoch 2 Loss 3.205\n",
      "adam: Epoch 4 Loss 2.970\n",
      "adam: Epoch 6 Loss 2.853\n",
      "adam: Epoch 8 Loss 2.804\n",
      "adam: Epoch 10 Loss 2.778\n",
      "adam: Epoch 12 Loss 2.760\n",
      "adam: Epoch 14 Loss 2.746\n",
      "adam: Epoch 16 Loss 2.736\n",
      "adam: Epoch 18 Loss 2.748\n",
      "adam: Epoch 20 Loss 2.718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Hand-drawn letters')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_hiddens_per_conv_layer = [10, 5]\n",
    "patch_size_per_conv_layer = [3, 2]\n",
    "stride_per_conv_layer=[1, 4]\n",
    "n_hiddens_per_fc_layer = [1]\n",
    "\n",
    "cnnet = CNN2D(16 * 16, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, len(np.unique(Ttrain)), \n",
    "              patch_size_per_conv_layer, stride_per_conv_layer, device=device)\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "cnnet.train(Xtrain, Ttrain, batch_size, n_epochs, learning_rate, method='adam')\n",
    "\n",
    "plt.plot(cnnet.error_trace, label='Pytorch')\n",
    "plt.title('Hand-drawn letters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Now we can run some experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments will be performed using different architectures and various hyperparameter values. We also use `time` to better understand the duration of training the models. The goal is to get the highest test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a methodical approach, instead of running experiments with multiple random parameters, let us only consider multiple values of a single parameter at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 1: Different network architectures for fully-connected layer**\n",
    "\n",
    "So, this first batch of experiments uses different architectures for fully connected layer. Rest of the parameters are constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully-connected layer: []\n",
      "adam: Epoch 2 Loss 1.440\n",
      "adam: Epoch 4 Loss 1.133\n",
      "adam: Epoch 6 Loss 0.996\n",
      "adam: Epoch 8 Loss 0.935\n",
      "adam: Epoch 10 Loss 0.877\n",
      "adam: Epoch 12 Loss 0.823\n",
      "adam: Epoch 14 Loss 0.793\n",
      "adam: Epoch 16 Loss 0.756\n",
      "adam: Epoch 18 Loss 0.730\n",
      "adam: Epoch 20 Loss 0.724\n",
      "Training took 2.6941168308258057 seconds.\n",
      "Train accuracy in percent correct: 82.76\n",
      "Test accuracy in percent correct: 80.13\n",
      "Test accuracy in percent correct: 80.13\n",
      "\n",
      "\n",
      "Fully-connected layer: [10]\n",
      "adam: Epoch 2 Loss 2.253\n",
      "adam: Epoch 4 Loss 1.701\n",
      "adam: Epoch 6 Loss 1.423\n",
      "adam: Epoch 8 Loss 1.251\n",
      "adam: Epoch 10 Loss 1.130\n",
      "adam: Epoch 12 Loss 1.044\n",
      "adam: Epoch 14 Loss 0.985\n",
      "adam: Epoch 16 Loss 0.934\n",
      "adam: Epoch 18 Loss 0.897\n",
      "adam: Epoch 20 Loss 0.871\n",
      "Training took 3.316105842590332 seconds.\n",
      "Train accuracy in percent correct: 76.99\n",
      "Test accuracy in percent correct: 74.48\n",
      "Test accuracy in percent correct: 74.48\n",
      "\n",
      "\n",
      "Fully-connected layer: [10, 10]\n",
      "adam: Epoch 2 Loss 2.392\n",
      "adam: Epoch 4 Loss 1.895\n",
      "adam: Epoch 6 Loss 1.621\n",
      "adam: Epoch 8 Loss 1.414\n",
      "adam: Epoch 10 Loss 1.287\n",
      "adam: Epoch 12 Loss 1.184\n",
      "adam: Epoch 14 Loss 1.086\n",
      "adam: Epoch 16 Loss 1.027\n",
      "adam: Epoch 18 Loss 0.987\n",
      "adam: Epoch 20 Loss 0.973\n",
      "Training took 3.9486427307128906 seconds.\n",
      "Train accuracy in percent correct: 74.68\n",
      "Test accuracy in percent correct: 72.21\n",
      "Test accuracy in percent correct: 72.21\n",
      "\n",
      "\n",
      "Fully-connected layer: [10, 10, 10]\n",
      "adam: Epoch 2 Loss 2.698\n",
      "adam: Epoch 4 Loss 2.157\n",
      "adam: Epoch 6 Loss 1.791\n",
      "adam: Epoch 8 Loss 1.557\n",
      "adam: Epoch 10 Loss 1.424\n",
      "adam: Epoch 12 Loss 1.321\n",
      "adam: Epoch 14 Loss 1.258\n",
      "adam: Epoch 16 Loss 1.214\n",
      "adam: Epoch 18 Loss 1.137\n",
      "adam: Epoch 20 Loss 1.102\n",
      "Training took 3.957353115081787 seconds.\n",
      "Train accuracy in percent correct: 72.91\n",
      "Test accuracy in percent correct: 70.89\n",
      "Test accuracy in percent correct: 70.89\n",
      "\n",
      "\n",
      "Fully-connected layer: [5, 5, 5]\n",
      "adam: Epoch 2 Loss 2.919\n",
      "adam: Epoch 4 Loss 2.468\n",
      "adam: Epoch 6 Loss 2.189\n",
      "adam: Epoch 8 Loss 1.959\n",
      "adam: Epoch 10 Loss 1.821\n",
      "adam: Epoch 12 Loss 1.687\n",
      "adam: Epoch 14 Loss 1.595\n",
      "adam: Epoch 16 Loss 1.558\n",
      "adam: Epoch 18 Loss 1.505\n",
      "adam: Epoch 20 Loss 1.454\n",
      "Training took 4.184251546859741 seconds.\n",
      "Train accuracy in percent correct: 59.92\n",
      "Test accuracy in percent correct: 58.60\n",
      "Test accuracy in percent correct: 58.60\n",
      "\n",
      "\n",
      "Fully-connected layer: [25, 20]\n",
      "adam: Epoch 2 Loss 2.169\n",
      "adam: Epoch 4 Loss 1.447\n",
      "adam: Epoch 6 Loss 1.169\n",
      "adam: Epoch 8 Loss 1.031\n",
      "adam: Epoch 10 Loss 0.924\n",
      "adam: Epoch 12 Loss 0.840\n",
      "adam: Epoch 14 Loss 0.791\n",
      "adam: Epoch 16 Loss 0.757\n",
      "adam: Epoch 18 Loss 0.721\n",
      "adam: Epoch 20 Loss 0.701\n",
      "Training took 3.3314011096954346 seconds.\n",
      "Train accuracy in percent correct: 81.62\n",
      "Test accuracy in percent correct: 78.51\n",
      "Test accuracy in percent correct: 78.51\n",
      "\n",
      "\n",
      "Fully-connected layer: [100]\n",
      "adam: Epoch 2 Loss 1.846\n",
      "adam: Epoch 4 Loss 1.167\n",
      "adam: Epoch 6 Loss 0.925\n",
      "adam: Epoch 8 Loss 0.859\n",
      "adam: Epoch 10 Loss 0.739\n",
      "adam: Epoch 12 Loss 0.696\n",
      "adam: Epoch 14 Loss 0.626\n",
      "adam: Epoch 16 Loss 0.587\n",
      "adam: Epoch 18 Loss 0.555\n",
      "adam: Epoch 20 Loss 0.530\n",
      "Training took 2.7123072147369385 seconds.\n",
      "Train accuracy in percent correct: 87.66\n",
      "Test accuracy in percent correct: 83.08\n",
      "Test accuracy in percent correct: 83.08\n",
      "\n",
      "\n",
      "Fully-connected layer: [200]\n",
      "adam: Epoch 2 Loss 2.671\n",
      "adam: Epoch 4 Loss 1.643\n",
      "adam: Epoch 6 Loss 1.036\n",
      "adam: Epoch 8 Loss 0.921\n",
      "adam: Epoch 10 Loss 0.827\n",
      "adam: Epoch 12 Loss 0.759\n",
      "adam: Epoch 14 Loss 0.691\n",
      "adam: Epoch 16 Loss 0.654\n",
      "adam: Epoch 18 Loss 0.595\n",
      "adam: Epoch 20 Loss 0.561\n",
      "Training took 2.100661516189575 seconds.\n",
      "Train accuracy in percent correct: 87.36\n",
      "Test accuracy in percent correct: 82.38\n",
      "Test accuracy in percent correct: 82.38\n",
      "\n",
      "\n",
      "Fully-connected layer: [80]\n",
      "adam: Epoch 2 Loss 1.786\n",
      "adam: Epoch 4 Loss 1.274\n",
      "adam: Epoch 6 Loss 0.970\n",
      "adam: Epoch 8 Loss 0.826\n",
      "adam: Epoch 10 Loss 0.763\n",
      "adam: Epoch 12 Loss 0.714\n",
      "adam: Epoch 14 Loss 0.639\n",
      "adam: Epoch 16 Loss 0.613\n",
      "adam: Epoch 18 Loss 0.572\n",
      "adam: Epoch 20 Loss 0.541\n",
      "Training took 2.150918960571289 seconds.\n",
      "Train accuracy in percent correct: 86.85\n",
      "Test accuracy in percent correct: 82.89\n",
      "Test accuracy in percent correct: 82.89\n",
      "\n",
      "\n",
      "Fully-connected layer: [70]\n",
      "adam: Epoch 2 Loss 1.641\n",
      "adam: Epoch 4 Loss 1.145\n",
      "adam: Epoch 6 Loss 0.925\n",
      "adam: Epoch 8 Loss 0.820\n",
      "adam: Epoch 10 Loss 0.800\n",
      "adam: Epoch 12 Loss 0.706\n",
      "adam: Epoch 14 Loss 0.655\n",
      "adam: Epoch 16 Loss 0.631\n",
      "adam: Epoch 18 Loss 0.569\n",
      "adam: Epoch 20 Loss 0.550\n",
      "Training took 2.1782562732696533 seconds.\n",
      "Train accuracy in percent correct: 86.05\n",
      "Test accuracy in percent correct: 82.38\n",
      "Test accuracy in percent correct: 82.38\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results1 = []\n",
    "n_epochs = 20\n",
    "batch_size = 500\n",
    "learning_rate = 0.01\n",
    "n_hiddens_per_conv_layer = [10,20]\n",
    "patch_size_per_conv_layer = [5,2]\n",
    "stride_per_conv_layer=[3,1]\n",
    "n_hiddens_per_fc_layer = [ [], [10], [10,10], [10,10,10], [5,5,5], [25,20], [100], [200], [80], [70]]\n",
    "\n",
    "for fc_layer in n_hiddens_per_fc_layer:\n",
    "    \n",
    "    print('Fully-connected layer:', fc_layer)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cnnet = CNN2D(16 * 16, n_hiddens_per_conv_layer, fc_layer, len(np.unique(Ttrain)), \n",
    "              patch_size_per_conv_layer, stride_per_conv_layer, device=device)\n",
    "    \n",
    "    cnnet.train(Xtrain, Ttrain, batch_size, n_epochs, learning_rate, method='adam')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    train_perc_correct,test_perc_correct,val_perc_correct=printstuff(Xtrain,Ttrain,Xtest,Ttest,Xval,Tval)\n",
    "    \n",
    "    experiment = [n_epochs,batch_size,learning_rate,n_hiddens_per_conv_layer, patch_size_per_conv_layer, stride_per_conv_layer, fc_layer, train_perc_correct, test_perc_correct, val_perc_correct, elapsed_time]\n",
    "    results1.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Hiddens per convolutional layer</th>\n",
       "      <th>Patch size per convolutional layer</th>\n",
       "      <th>Stride per convolutional layer</th>\n",
       "      <th>Hiddens per fully-connected layer</th>\n",
       "      <th>Train percent correct</th>\n",
       "      <th>Test percent correct</th>\n",
       "      <th>Validate percent correct</th>\n",
       "      <th>Training time (seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10, 20]</td>\n",
       "      <td>[5, 2]</td>\n",
       "      <td>[3, 1]</td>\n",
       "      <td>[5, 5, 5]</td>\n",
       "      <td>59.921446</td>\n",
       "      <td>58.600499</td>\n",
       "      <td>58.552505</td>\n",
       "      <td>4.184252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10, 20]</td>\n",
       "      <td>[5, 2]</td>\n",
       "      <td>[3, 1]</td>\n",
       "      <td>[10, 10, 10]</td>\n",
       "      <td>72.908417</td>\n",
       "      <td>70.886926</td>\n",
       "      <td>71.174890</td>\n",
       "      <td>3.957353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10, 20]</td>\n",
       "      <td>[5, 2]</td>\n",
       "      <td>[3, 1]</td>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>74.683868</td>\n",
       "      <td>72.211557</td>\n",
       "      <td>72.192359</td>\n",
       "      <td>3.948643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10, 20]</td>\n",
       "      <td>[5, 2]</td>\n",
       "      <td>[3, 1]</td>\n",
       "      <td>[10]</td>\n",
       "      <td>76.989398</td>\n",
       "      <td>74.476867</td>\n",
       "      <td>75.062392</td>\n",
       "      <td>3.316106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10, 20]</td>\n",
       "      <td>[5, 2]</td>\n",
       "      <td>[3, 1]</td>\n",
       "      <td>[25, 20]</td>\n",
       "      <td>81.616426</td>\n",
       "      <td>78.508351</td>\n",
       "      <td>78.076406</td>\n",
       "      <td>3.331401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10, 20]</td>\n",
       "      <td>[5, 2]</td>\n",
       "      <td>[3, 1]</td>\n",
       "      <td>[]</td>\n",
       "      <td>82.762805</td>\n",
       "      <td>80.130543</td>\n",
       "      <td>80.216932</td>\n",
       "      <td>2.694117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10, 20]</td>\n",
       "      <td>[5, 2]</td>\n",
       "      <td>[3, 1]</td>\n",
       "      <td>[200]</td>\n",
       "      <td>87.361093</td>\n",
       "      <td>82.376656</td>\n",
       "      <td>82.559032</td>\n",
       "      <td>2.100662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10, 20]</td>\n",
       "      <td>[5, 2]</td>\n",
       "      <td>[3, 1]</td>\n",
       "      <td>[70]</td>\n",
       "      <td>86.045472</td>\n",
       "      <td>82.376656</td>\n",
       "      <td>82.415051</td>\n",
       "      <td>2.178256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10, 20]</td>\n",
       "      <td>[5, 2]</td>\n",
       "      <td>[3, 1]</td>\n",
       "      <td>[80]</td>\n",
       "      <td>86.846979</td>\n",
       "      <td>82.885391</td>\n",
       "      <td>82.597428</td>\n",
       "      <td>2.150919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10, 20]</td>\n",
       "      <td>[5, 2]</td>\n",
       "      <td>[3, 1]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>87.664453</td>\n",
       "      <td>83.077366</td>\n",
       "      <td>83.211749</td>\n",
       "      <td>2.712307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epochs  Batch size  Learning Rate Hiddens per convolutional layer  \\\n",
       "4      20         500           0.01                        [10, 20]   \n",
       "3      20         500           0.01                        [10, 20]   \n",
       "2      20         500           0.01                        [10, 20]   \n",
       "1      20         500           0.01                        [10, 20]   \n",
       "5      20         500           0.01                        [10, 20]   \n",
       "0      20         500           0.01                        [10, 20]   \n",
       "7      20         500           0.01                        [10, 20]   \n",
       "9      20         500           0.01                        [10, 20]   \n",
       "8      20         500           0.01                        [10, 20]   \n",
       "6      20         500           0.01                        [10, 20]   \n",
       "\n",
       "  Patch size per convolutional layer Stride per convolutional layer  \\\n",
       "4                             [5, 2]                         [3, 1]   \n",
       "3                             [5, 2]                         [3, 1]   \n",
       "2                             [5, 2]                         [3, 1]   \n",
       "1                             [5, 2]                         [3, 1]   \n",
       "5                             [5, 2]                         [3, 1]   \n",
       "0                             [5, 2]                         [3, 1]   \n",
       "7                             [5, 2]                         [3, 1]   \n",
       "9                             [5, 2]                         [3, 1]   \n",
       "8                             [5, 2]                         [3, 1]   \n",
       "6                             [5, 2]                         [3, 1]   \n",
       "\n",
       "  Hiddens per fully-connected layer  Train percent correct  \\\n",
       "4                         [5, 5, 5]              59.921446   \n",
       "3                      [10, 10, 10]              72.908417   \n",
       "2                          [10, 10]              74.683868   \n",
       "1                              [10]              76.989398   \n",
       "5                          [25, 20]              81.616426   \n",
       "0                                []              82.762805   \n",
       "7                             [200]              87.361093   \n",
       "9                              [70]              86.045472   \n",
       "8                              [80]              86.846979   \n",
       "6                             [100]              87.664453   \n",
       "\n",
       "   Test percent correct  Validate percent correct  Training time (seconds)  \n",
       "4             58.600499                 58.552505                 4.184252  \n",
       "3             70.886926                 71.174890                 3.957353  \n",
       "2             72.211557                 72.192359                 3.948643  \n",
       "1             74.476867                 75.062392                 3.316106  \n",
       "5             78.508351                 78.076406                 3.331401  \n",
       "0             80.130543                 80.216932                 2.694117  \n",
       "7             82.376656                 82.559032                 2.100662  \n",
       "9             82.376656                 82.415051                 2.178256  \n",
       "8             82.885391                 82.597428                 2.150919  \n",
       "6             83.077366                 83.211749                 2.712307  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP1 = pd.DataFrame(results1, columns=names)\n",
    "EXP1.sort_values('Test percent correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table we can see that, when all the other parameters are constant and only the network architecture of fully-connected layer is changed, a single layer gives higher accuracy on test set than two or three layers. Further, we can see that, using 100 neurons in this single layer gives highest accuracy on the test set compared to other single layer neuron values. Using 200 neurons might be overfitting the model and hence we get less accuracy while using linear layer gives even less value for accuracy on the test set. So, we go forward with keeping the network architecture of the fully-connected layer as [100]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 2: Different network architectures for convolutional layer**\n",
    "\n",
    "Now, we change only the network architecture for the convolutional layer. Rest of the parameters are constant. For better understanding the patch sizes and strides are kept as 2 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiddens per convolutional layer: [10]\n",
      "Patch size per convolutional layer: [2]\n",
      "Stride per convolutional layer: [2]\n",
      "adam: Epoch 2 Loss 2.161\n",
      "adam: Epoch 4 Loss 1.121\n",
      "adam: Epoch 6 Loss 0.843\n",
      "adam: Epoch 8 Loss 0.794\n",
      "adam: Epoch 10 Loss 0.667\n",
      "adam: Epoch 12 Loss 0.572\n",
      "adam: Epoch 14 Loss 0.532\n",
      "adam: Epoch 16 Loss 0.493\n",
      "adam: Epoch 18 Loss 0.446\n",
      "adam: Epoch 20 Loss 0.412\n",
      "Training took 2.7145917415618896 seconds.\n",
      "Train accuracy in percent correct: 90.18\n",
      "Test accuracy in percent correct: 84.50\n",
      "Test accuracy in percent correct: 84.50\n",
      "\n",
      "\n",
      "Hiddens per convolutional layer: [10, 10]\n",
      "Patch size per convolutional layer: [2, 2]\n",
      "Stride per convolutional layer: [2, 2]\n",
      "adam: Epoch 2 Loss 4.262\n",
      "adam: Epoch 4 Loss 3.984\n",
      "adam: Epoch 6 Loss 2.856\n",
      "adam: Epoch 8 Loss 1.728\n",
      "adam: Epoch 10 Loss 1.368\n",
      "adam: Epoch 12 Loss 1.069\n",
      "adam: Epoch 14 Loss 0.869\n",
      "adam: Epoch 16 Loss 0.811\n",
      "adam: Epoch 18 Loss 0.747\n",
      "adam: Epoch 20 Loss 0.688\n",
      "Training took 2.3148608207702637 seconds.\n",
      "Train accuracy in percent correct: 83.07\n",
      "Test accuracy in percent correct: 81.18\n",
      "Test accuracy in percent correct: 81.18\n",
      "\n",
      "\n",
      "Hiddens per convolutional layer: [10, 10, 10]\n",
      "Patch size per convolutional layer: [2, 2, 2]\n",
      "Stride per convolutional layer: [2, 2, 2]\n",
      "adam: Epoch 2 Loss 4.357\n",
      "adam: Epoch 4 Loss 4.215\n",
      "adam: Epoch 6 Loss 4.014\n",
      "adam: Epoch 8 Loss 3.721\n",
      "adam: Epoch 10 Loss 2.415\n",
      "adam: Epoch 12 Loss 1.556\n",
      "adam: Epoch 14 Loss 1.294\n",
      "adam: Epoch 16 Loss 1.092\n",
      "adam: Epoch 18 Loss 0.961\n",
      "adam: Epoch 20 Loss 0.888\n",
      "Training took 2.6455438137054443 seconds.\n",
      "Train accuracy in percent correct: 77.98\n",
      "Test accuracy in percent correct: 76.83\n",
      "Test accuracy in percent correct: 76.83\n",
      "\n",
      "\n",
      "Hiddens per convolutional layer: [50, 50, 50]\n",
      "Patch size per convolutional layer: [2, 2, 2]\n",
      "Stride per convolutional layer: [2, 2, 2]\n",
      "adam: Epoch 2 Loss 4.199\n",
      "adam: Epoch 4 Loss 4.170\n",
      "adam: Epoch 6 Loss 4.233\n",
      "adam: Epoch 8 Loss 3.957\n",
      "adam: Epoch 10 Loss 2.956\n",
      "adam: Epoch 12 Loss 2.512\n",
      "adam: Epoch 14 Loss 2.099\n",
      "adam: Epoch 16 Loss 1.738\n",
      "adam: Epoch 18 Loss 1.415\n",
      "adam: Epoch 20 Loss 1.403\n",
      "Training took 2.997619152069092 seconds.\n",
      "Train accuracy in percent correct: 68.61\n",
      "Test accuracy in percent correct: 67.95\n",
      "Test accuracy in percent correct: 67.95\n",
      "\n",
      "\n",
      "Hiddens per convolutional layer: [30, 20]\n",
      "Patch size per convolutional layer: [2, 2]\n",
      "Stride per convolutional layer: [2, 2]\n",
      "adam: Epoch 2 Loss 4.222\n",
      "adam: Epoch 4 Loss 4.180\n",
      "adam: Epoch 6 Loss 4.179\n",
      "adam: Epoch 8 Loss 4.178\n",
      "adam: Epoch 10 Loss 4.184\n",
      "adam: Epoch 12 Loss 4.175\n",
      "adam: Epoch 14 Loss 4.177\n",
      "adam: Epoch 16 Loss 4.177\n",
      "adam: Epoch 18 Loss 4.169\n",
      "adam: Epoch 20 Loss 4.089\n",
      "Training took 2.077134132385254 seconds.\n",
      "Train accuracy in percent correct: 22.80\n",
      "Test accuracy in percent correct: 23.09\n",
      "Test accuracy in percent correct: 23.09\n",
      "\n",
      "\n",
      "Hiddens per convolutional layer: [100]\n",
      "Patch size per convolutional layer: [2]\n",
      "Stride per convolutional layer: [2]\n",
      "adam: Epoch 2 Loss 4.124\n",
      "adam: Epoch 4 Loss 4.160\n",
      "adam: Epoch 6 Loss 4.166\n",
      "adam: Epoch 8 Loss 4.169\n",
      "adam: Epoch 10 Loss 4.171\n",
      "adam: Epoch 12 Loss 4.172\n",
      "adam: Epoch 14 Loss 4.173\n",
      "adam: Epoch 16 Loss 4.173\n",
      "adam: Epoch 18 Loss 4.174\n",
      "adam: Epoch 20 Loss 4.174\n",
      "Training took 1.6265692710876465 seconds.\n",
      "Train accuracy in percent correct: 9.63\n",
      "Test accuracy in percent correct: 9.64\n",
      "Test accuracy in percent correct: 9.64\n",
      "\n",
      "\n",
      "Hiddens per convolutional layer: [200]\n",
      "Patch size per convolutional layer: [2]\n",
      "Stride per convolutional layer: [2]\n",
      "adam: Epoch 2 Loss 4.120\n",
      "adam: Epoch 4 Loss 4.159\n",
      "adam: Epoch 6 Loss 4.165\n",
      "adam: Epoch 8 Loss 4.169\n",
      "adam: Epoch 10 Loss 4.171\n",
      "adam: Epoch 12 Loss 4.172\n",
      "adam: Epoch 14 Loss 4.173\n",
      "adam: Epoch 16 Loss 4.173\n",
      "adam: Epoch 18 Loss 4.174\n",
      "adam: Epoch 20 Loss 4.174\n",
      "Training took 2.0491433143615723 seconds.\n",
      "Train accuracy in percent correct: 9.63\n",
      "Test accuracy in percent correct: 9.64\n",
      "Test accuracy in percent correct: 9.64\n",
      "\n",
      "\n",
      "Hiddens per convolutional layer: [500]\n",
      "Patch size per convolutional layer: [2]\n",
      "Stride per convolutional layer: [2]\n",
      "adam: Epoch 2 Loss 4.122\n",
      "adam: Epoch 4 Loss 4.159\n",
      "adam: Epoch 6 Loss 4.166\n",
      "adam: Epoch 8 Loss 4.169\n",
      "adam: Epoch 10 Loss 4.171\n",
      "adam: Epoch 12 Loss 4.172\n",
      "adam: Epoch 14 Loss 4.173\n",
      "adam: Epoch 16 Loss 4.173\n",
      "adam: Epoch 18 Loss 4.174\n",
      "adam: Epoch 20 Loss 4.174\n",
      "Training took 3.2485921382904053 seconds.\n",
      "Train accuracy in percent correct: 9.63\n",
      "Test accuracy in percent correct: 9.64\n",
      "Test accuracy in percent correct: 9.64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results2 = []\n",
    "n_epochs = 20\n",
    "batch_size = 500\n",
    "learning_rate = 0.01\n",
    "n_hiddens_per_conv_layer = [[10],[10,10],[10,10,10],[50,50,50],[30,20],[100],[200],[500]]\n",
    "patch_size_per_conv_layer = [[2],[2,2],[2,2,2],[2,2,2],[2,2],[2],[2],[2]]\n",
    "stride_per_conv_layer=[[2],[2,2],[2,2,2],[2,2,2],[2,2],[2],[2],[2]]\n",
    "n_hiddens_per_fc_layer = [100]\n",
    "\n",
    "for i in range(8):\n",
    "    \n",
    "    print('Hiddens per convolutional layer:', n_hiddens_per_conv_layer[i])\n",
    "    print('Patch size per convolutional layer:', patch_size_per_conv_layer[i])\n",
    "    print('Stride per convolutional layer:', stride_per_conv_layer[i])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cnnet = CNN2D(16 * 16, n_hiddens_per_conv_layer[i], n_hiddens_per_fc_layer, len(np.unique(Ttrain)), \n",
    "              patch_size_per_conv_layer[i], stride_per_conv_layer[i], device=device)\n",
    "    \n",
    "    cnnet.train(Xtrain, Ttrain, batch_size, n_epochs, learning_rate, method='adam')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    train_perc_correct,test_perc_correct,val_perc_correct=printstuff(Xtrain,Ttrain,Xtest,Ttest,Xval,Tval)\n",
    "    \n",
    "    experiment = [n_epochs,batch_size,learning_rate,n_hiddens_per_conv_layer[i], patch_size_per_conv_layer[i], stride_per_conv_layer[i], n_hiddens_per_fc_layer, train_perc_correct, test_perc_correct, val_perc_correct, elapsed_time]\n",
    "    results2.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Hiddens per convolutional layer</th>\n",
       "      <th>Patch size per convolutional layer</th>\n",
       "      <th>Stride per convolutional layer</th>\n",
       "      <th>Hiddens per fully-connected layer</th>\n",
       "      <th>Train percent correct</th>\n",
       "      <th>Test percent correct</th>\n",
       "      <th>Validate percent correct</th>\n",
       "      <th>Training time (seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[100]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>9.630860</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>1.626569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[200]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>9.630860</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>2.049143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[500]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>9.630860</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>3.248592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[30, 20]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>22.799847</td>\n",
       "      <td>23.085045</td>\n",
       "      <td>23.104243</td>\n",
       "      <td>2.077134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[50, 50, 50]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>68.613488</td>\n",
       "      <td>67.949702</td>\n",
       "      <td>68.180073</td>\n",
       "      <td>2.997619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10, 10, 10]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>77.979308</td>\n",
       "      <td>76.828566</td>\n",
       "      <td>76.617393</td>\n",
       "      <td>2.645544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>83.066164</td>\n",
       "      <td>81.176809</td>\n",
       "      <td>81.100019</td>\n",
       "      <td>2.314861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>90.180738</td>\n",
       "      <td>84.497984</td>\n",
       "      <td>84.545978</td>\n",
       "      <td>2.714592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epochs  Batch size  Learning Rate Hiddens per convolutional layer  \\\n",
       "5      20         500           0.01                           [100]   \n",
       "6      20         500           0.01                           [200]   \n",
       "7      20         500           0.01                           [500]   \n",
       "4      20         500           0.01                        [30, 20]   \n",
       "3      20         500           0.01                    [50, 50, 50]   \n",
       "2      20         500           0.01                    [10, 10, 10]   \n",
       "1      20         500           0.01                        [10, 10]   \n",
       "0      20         500           0.01                            [10]   \n",
       "\n",
       "  Patch size per convolutional layer Stride per convolutional layer  \\\n",
       "5                                [2]                            [2]   \n",
       "6                                [2]                            [2]   \n",
       "7                                [2]                            [2]   \n",
       "4                             [2, 2]                         [2, 2]   \n",
       "3                          [2, 2, 2]                      [2, 2, 2]   \n",
       "2                          [2, 2, 2]                      [2, 2, 2]   \n",
       "1                             [2, 2]                         [2, 2]   \n",
       "0                                [2]                            [2]   \n",
       "\n",
       "  Hiddens per fully-connected layer  Train percent correct  \\\n",
       "5                             [100]               9.630860   \n",
       "6                             [100]               9.630860   \n",
       "7                             [100]               9.630860   \n",
       "4                             [100]              22.799847   \n",
       "3                             [100]              68.613488   \n",
       "2                             [100]              77.979308   \n",
       "1                             [100]              83.066164   \n",
       "0                             [100]              90.180738   \n",
       "\n",
       "   Test percent correct  Validate percent correct  Training time (seconds)  \n",
       "5              9.637166                  9.637166                 1.626569  \n",
       "6              9.637166                  9.637166                 2.049143  \n",
       "7              9.637166                  9.637166                 3.248592  \n",
       "4             23.085045                 23.104243                 2.077134  \n",
       "3             67.949702                 68.180073                 2.997619  \n",
       "2             76.828566                 76.617393                 2.645544  \n",
       "1             81.176809                 81.100019                 2.314861  \n",
       "0             84.497984                 84.545978                 2.714592  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP2 = pd.DataFrame(results2, columns=names)\n",
    "EXP2.sort_values('Test percent correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table we can see that, when all the other parameters are constant and only the network architecture of convolutional layer is changed, a single layer gives higher accuracy on test set than two or three layers. A very unique observation here is that, if in a single layer the neurons are kept at a very high value like 100 or 200 or 500, we get accuracy value of around 9%, much less than when we use a two or a three layer network architecture for this layer.  So, we go forward with keeping the network architecture of the convolutional layer as [10]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 3: Different neurons for a single layer convolutional layer**\n",
    "\n",
    "Now, we test with different neurons for the single layer convolutional layer. Rest of the parameters are constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutional layer: [10]\n",
      "adam: Epoch 2 Loss 1.633\n",
      "adam: Epoch 4 Loss 1.077\n",
      "adam: Epoch 6 Loss 0.761\n",
      "adam: Epoch 8 Loss 0.662\n",
      "adam: Epoch 10 Loss 0.623\n",
      "adam: Epoch 12 Loss 0.520\n",
      "adam: Epoch 14 Loss 0.470\n",
      "adam: Epoch 16 Loss 0.439\n",
      "adam: Epoch 18 Loss 0.409\n",
      "adam: Epoch 20 Loss 0.382\n",
      "Training took 3.1370670795440674 seconds.\n",
      "Train accuracy in percent correct: 91.26\n",
      "Test accuracy in percent correct: 84.46\n",
      "Test accuracy in percent correct: 84.46\n",
      "\n",
      "\n",
      "Convolutional layer: [30]\n",
      "adam: Epoch 2 Loss 4.124\n",
      "adam: Epoch 4 Loss 4.159\n",
      "adam: Epoch 6 Loss 4.166\n",
      "adam: Epoch 8 Loss 4.169\n",
      "adam: Epoch 10 Loss 4.171\n",
      "adam: Epoch 12 Loss 4.172\n",
      "adam: Epoch 14 Loss 4.173\n",
      "adam: Epoch 16 Loss 4.173\n",
      "adam: Epoch 18 Loss 4.174\n",
      "adam: Epoch 20 Loss 4.174\n",
      "Training took 2.8583619594573975 seconds.\n",
      "Train accuracy in percent correct: 9.63\n",
      "Test accuracy in percent correct: 9.64\n",
      "Test accuracy in percent correct: 9.64\n",
      "\n",
      "\n",
      "Convolutional layer: [50]\n",
      "adam: Epoch 2 Loss 4.120\n",
      "adam: Epoch 4 Loss 4.158\n",
      "adam: Epoch 6 Loss 4.165\n",
      "adam: Epoch 8 Loss 4.169\n",
      "adam: Epoch 10 Loss 4.171\n",
      "adam: Epoch 12 Loss 4.172\n",
      "adam: Epoch 14 Loss 4.173\n",
      "adam: Epoch 16 Loss 4.173\n",
      "adam: Epoch 18 Loss 4.174\n",
      "adam: Epoch 20 Loss 4.174\n",
      "Training took 1.6597211360931396 seconds.\n",
      "Train accuracy in percent correct: 9.63\n",
      "Test accuracy in percent correct: 9.64\n",
      "Test accuracy in percent correct: 9.64\n",
      "\n",
      "\n",
      "Convolutional layer: [5]\n",
      "adam: Epoch 2 Loss 2.268\n",
      "adam: Epoch 4 Loss 1.125\n",
      "adam: Epoch 6 Loss 0.840\n",
      "adam: Epoch 8 Loss 0.795\n",
      "adam: Epoch 10 Loss 0.657\n",
      "adam: Epoch 12 Loss 0.603\n",
      "adam: Epoch 14 Loss 0.564\n",
      "adam: Epoch 16 Loss 0.511\n",
      "adam: Epoch 18 Loss 0.476\n",
      "adam: Epoch 20 Loss 0.449\n",
      "Training took 2.3514978885650635 seconds.\n",
      "Train accuracy in percent correct: 89.58\n",
      "Test accuracy in percent correct: 84.60\n",
      "Test accuracy in percent correct: 84.60\n",
      "\n",
      "\n",
      "Convolutional layer: [1]\n",
      "adam: Epoch 2 Loss 1.634\n",
      "adam: Epoch 4 Loss 1.235\n",
      "adam: Epoch 6 Loss 1.078\n",
      "adam: Epoch 8 Loss 1.007\n",
      "adam: Epoch 10 Loss 0.941\n",
      "adam: Epoch 12 Loss 0.875\n",
      "adam: Epoch 14 Loss 0.824\n",
      "adam: Epoch 16 Loss 0.781\n",
      "adam: Epoch 18 Loss 0.757\n",
      "adam: Epoch 20 Loss 0.712\n",
      "Training took 1.8303885459899902 seconds.\n",
      "Train accuracy in percent correct: 82.58\n",
      "Test accuracy in percent correct: 80.74\n",
      "Test accuracy in percent correct: 80.74\n",
      "\n",
      "\n",
      "Convolutional layer: [100]\n",
      "adam: Epoch 2 Loss 4.118\n",
      "adam: Epoch 4 Loss 4.158\n",
      "adam: Epoch 6 Loss 4.165\n",
      "adam: Epoch 8 Loss 4.168\n",
      "adam: Epoch 10 Loss 4.170\n",
      "adam: Epoch 12 Loss 4.172\n",
      "adam: Epoch 14 Loss 4.172\n",
      "adam: Epoch 16 Loss 4.173\n",
      "adam: Epoch 18 Loss 4.174\n",
      "adam: Epoch 20 Loss 4.174\n",
      "Training took 1.6458873748779297 seconds.\n",
      "Train accuracy in percent correct: 9.63\n",
      "Test accuracy in percent correct: 9.64\n",
      "Test accuracy in percent correct: 9.64\n",
      "\n",
      "\n",
      "Convolutional layer: [200]\n",
      "adam: Epoch 2 Loss 4.120\n",
      "adam: Epoch 4 Loss 4.159\n",
      "adam: Epoch 6 Loss 4.165\n",
      "adam: Epoch 8 Loss 4.169\n",
      "adam: Epoch 10 Loss 4.171\n",
      "adam: Epoch 12 Loss 4.172\n",
      "adam: Epoch 14 Loss 4.173\n",
      "adam: Epoch 16 Loss 4.173\n",
      "adam: Epoch 18 Loss 4.174\n",
      "adam: Epoch 20 Loss 4.174\n",
      "Training took 1.7557346820831299 seconds.\n",
      "Train accuracy in percent correct: 9.63\n",
      "Test accuracy in percent correct: 9.64\n",
      "Test accuracy in percent correct: 9.64\n",
      "\n",
      "\n",
      "Convolutional layer: [500]\n",
      "adam: Epoch 2 Loss 4.122\n",
      "adam: Epoch 4 Loss 4.160\n",
      "adam: Epoch 6 Loss 4.166\n",
      "adam: Epoch 8 Loss 4.169\n",
      "adam: Epoch 10 Loss 4.171\n",
      "adam: Epoch 12 Loss 4.172\n",
      "adam: Epoch 14 Loss 4.173\n",
      "adam: Epoch 16 Loss 4.173\n",
      "adam: Epoch 18 Loss 4.174\n",
      "adam: Epoch 20 Loss 4.174\n",
      "Training took 3.2491981983184814 seconds.\n",
      "Train accuracy in percent correct: 9.63\n",
      "Test accuracy in percent correct: 9.64\n",
      "Test accuracy in percent correct: 9.64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results3 = []\n",
    "n_epochs = 20\n",
    "batch_size = 500\n",
    "learning_rate = 0.01\n",
    "n_hiddens_per_conv_layer = [[10],[30],[50],[5],[1],[100],[200],[500]]\n",
    "patch_size_per_conv_layer = [2]\n",
    "stride_per_conv_layer=[2]\n",
    "n_hiddens_per_fc_layer = [100]\n",
    "\n",
    "for conv_layer in n_hiddens_per_conv_layer:\n",
    "    \n",
    "    print('Convolutional layer:', conv_layer)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cnnet = CNN2D(16 * 16, conv_layer, n_hiddens_per_fc_layer, len(np.unique(Ttrain)), \n",
    "              patch_size_per_conv_layer, stride_per_conv_layer, device=device)\n",
    "    \n",
    "    cnnet.train(Xtrain, Ttrain, batch_size, n_epochs, learning_rate, method='adam')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    train_perc_correct,test_perc_correct,val_perc_correct=printstuff(Xtrain,Ttrain,Xtest,Ttest,Xval,Tval)\n",
    "    \n",
    "    experiment = [n_epochs,batch_size,learning_rate,conv_layer, patch_size_per_conv_layer, stride_per_conv_layer, n_hiddens_per_fc_layer, train_perc_correct, test_perc_correct, val_perc_correct, elapsed_time]\n",
    "    results3.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Hiddens per convolutional layer</th>\n",
       "      <th>Patch size per convolutional layer</th>\n",
       "      <th>Stride per convolutional layer</th>\n",
       "      <th>Hiddens per fully-connected layer</th>\n",
       "      <th>Train percent correct</th>\n",
       "      <th>Test percent correct</th>\n",
       "      <th>Validate percent correct</th>\n",
       "      <th>Training time (seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[30]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>9.630860</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>2.858362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[50]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>9.630860</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>1.659721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[100]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>9.630860</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>1.645887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[200]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>9.630860</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>1.755735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[500]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>9.630860</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>3.249198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>82.580789</td>\n",
       "      <td>80.735266</td>\n",
       "      <td>80.466500</td>\n",
       "      <td>1.830389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>91.260059</td>\n",
       "      <td>84.459589</td>\n",
       "      <td>84.661163</td>\n",
       "      <td>3.137067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>89.583599</td>\n",
       "      <td>84.603571</td>\n",
       "      <td>84.929929</td>\n",
       "      <td>2.351498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epochs  Batch size  Learning Rate Hiddens per convolutional layer  \\\n",
       "1      20         500           0.01                            [30]   \n",
       "2      20         500           0.01                            [50]   \n",
       "5      20         500           0.01                           [100]   \n",
       "6      20         500           0.01                           [200]   \n",
       "7      20         500           0.01                           [500]   \n",
       "4      20         500           0.01                             [1]   \n",
       "0      20         500           0.01                            [10]   \n",
       "3      20         500           0.01                             [5]   \n",
       "\n",
       "  Patch size per convolutional layer Stride per convolutional layer  \\\n",
       "1                                [2]                            [2]   \n",
       "2                                [2]                            [2]   \n",
       "5                                [2]                            [2]   \n",
       "6                                [2]                            [2]   \n",
       "7                                [2]                            [2]   \n",
       "4                                [2]                            [2]   \n",
       "0                                [2]                            [2]   \n",
       "3                                [2]                            [2]   \n",
       "\n",
       "  Hiddens per fully-connected layer  Train percent correct  \\\n",
       "1                             [100]               9.630860   \n",
       "2                             [100]               9.630860   \n",
       "5                             [100]               9.630860   \n",
       "6                             [100]               9.630860   \n",
       "7                             [100]               9.630860   \n",
       "4                             [100]              82.580789   \n",
       "0                             [100]              91.260059   \n",
       "3                             [100]              89.583599   \n",
       "\n",
       "   Test percent correct  Validate percent correct  Training time (seconds)  \n",
       "1              9.637166                  9.637166                 2.858362  \n",
       "2              9.637166                  9.637166                 1.659721  \n",
       "5              9.637166                  9.637166                 1.645887  \n",
       "6              9.637166                  9.637166                 1.755735  \n",
       "7              9.637166                  9.637166                 3.249198  \n",
       "4             80.735266                 80.466500                 1.830389  \n",
       "0             84.459589                 84.661163                 3.137067  \n",
       "3             84.603571                 84.929929                 2.351498  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP3 = pd.DataFrame(results3, columns=names)\n",
    "EXP3.sort_values('Test percent correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed in experiment 2, in a single layer convolutional layer, if the neurons are kept at a high value like 30 or 50 or 100 or 200 or 500, we get very less accuracy value of around 9%. Using five neurons gives the highest accuracy for the test set. So, we go forward with keeping the network architecture of the convolutional layer as [5]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 4: Different patch sizes for the convolutional layer**\n",
    "\n",
    "Now, we test with different patch sizes for the convolutional layer. Rest of the parameters are constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch size: [10]\n",
      "adam: Epoch 2 Loss 1.417\n",
      "adam: Epoch 4 Loss 1.108\n",
      "adam: Epoch 6 Loss 0.857\n",
      "adam: Epoch 8 Loss 0.788\n",
      "adam: Epoch 10 Loss 0.702\n",
      "adam: Epoch 12 Loss 0.664\n",
      "adam: Epoch 14 Loss 0.614\n",
      "adam: Epoch 16 Loss 0.589\n",
      "adam: Epoch 18 Loss 0.552\n",
      "adam: Epoch 20 Loss 0.532\n",
      "Training took 1.5657432079315186 seconds.\n",
      "Train accuracy in percent correct: 87.13\n",
      "Test accuracy in percent correct: 82.97\n",
      "Test accuracy in percent correct: 82.97\n",
      "\n",
      "\n",
      "Patch size: [15]\n",
      "adam: Epoch 2 Loss 2.090\n",
      "adam: Epoch 4 Loss 1.751\n",
      "adam: Epoch 6 Loss 1.682\n",
      "adam: Epoch 8 Loss 1.664\n",
      "adam: Epoch 10 Loss 1.605\n",
      "adam: Epoch 12 Loss 1.585\n",
      "adam: Epoch 14 Loss 1.558\n",
      "adam: Epoch 16 Loss 1.528\n",
      "adam: Epoch 18 Loss 1.505\n",
      "adam: Epoch 20 Loss 1.485\n",
      "Training took 1.5895285606384277 seconds.\n",
      "Train accuracy in percent correct: 61.38\n",
      "Test accuracy in percent correct: 60.86\n",
      "Test accuracy in percent correct: 60.86\n",
      "\n",
      "\n",
      "Patch size: [16]\n",
      "adam: Epoch 2 Loss 2.006\n",
      "adam: Epoch 4 Loss 1.652\n",
      "adam: Epoch 6 Loss 1.575\n",
      "adam: Epoch 8 Loss 1.557\n",
      "adam: Epoch 10 Loss 1.493\n",
      "adam: Epoch 12 Loss 1.565\n",
      "adam: Epoch 14 Loss 1.457\n",
      "adam: Epoch 16 Loss 1.480\n",
      "adam: Epoch 18 Loss 1.425\n",
      "adam: Epoch 20 Loss 1.485\n",
      "Training took 1.502309799194336 seconds.\n",
      "Train accuracy in percent correct: 62.78\n",
      "Test accuracy in percent correct: 61.86\n",
      "Test accuracy in percent correct: 61.86\n",
      "\n",
      "\n",
      "Patch size: [4]\n",
      "adam: Epoch 2 Loss 1.525\n",
      "adam: Epoch 4 Loss 0.986\n",
      "adam: Epoch 6 Loss 0.791\n",
      "adam: Epoch 8 Loss 0.720\n",
      "adam: Epoch 10 Loss 0.631\n",
      "adam: Epoch 12 Loss 0.579\n",
      "adam: Epoch 14 Loss 0.526\n",
      "adam: Epoch 16 Loss 0.492\n",
      "adam: Epoch 18 Loss 0.457\n",
      "adam: Epoch 20 Loss 0.430\n",
      "Training took 1.512727975845337 seconds.\n",
      "Train accuracy in percent correct: 89.91\n",
      "Test accuracy in percent correct: 84.48\n",
      "Test accuracy in percent correct: 84.48\n",
      "\n",
      "\n",
      "Patch size: [5]\n",
      "adam: Epoch 2 Loss 1.344\n",
      "adam: Epoch 4 Loss 1.214\n",
      "adam: Epoch 6 Loss 0.840\n",
      "adam: Epoch 8 Loss 0.692\n",
      "adam: Epoch 10 Loss 0.623\n",
      "adam: Epoch 12 Loss 0.568\n",
      "adam: Epoch 14 Loss 0.568\n",
      "adam: Epoch 16 Loss 0.687\n",
      "adam: Epoch 18 Loss 0.458\n",
      "adam: Epoch 20 Loss 0.426\n",
      "Training took 1.5090436935424805 seconds.\n",
      "Train accuracy in percent correct: 89.47\n",
      "Test accuracy in percent correct: 83.55\n",
      "Test accuracy in percent correct: 83.55\n",
      "\n",
      "\n",
      "Patch size: [6]\n",
      "adam: Epoch 2 Loss 1.350\n",
      "adam: Epoch 4 Loss 0.970\n",
      "adam: Epoch 6 Loss 0.789\n",
      "adam: Epoch 8 Loss 0.721\n",
      "adam: Epoch 10 Loss 0.613\n",
      "adam: Epoch 12 Loss 0.554\n",
      "adam: Epoch 14 Loss 0.504\n",
      "adam: Epoch 16 Loss 0.464\n",
      "adam: Epoch 18 Loss 0.430\n",
      "adam: Epoch 20 Loss 0.403\n",
      "Training took 1.518693447113037 seconds.\n",
      "Train accuracy in percent correct: 90.59\n",
      "Test accuracy in percent correct: 84.23\n",
      "Test accuracy in percent correct: 84.23\n",
      "\n",
      "\n",
      "Patch size: [12]\n",
      "adam: Epoch 2 Loss 1.479\n",
      "adam: Epoch 4 Loss 1.383\n",
      "adam: Epoch 6 Loss 1.092\n",
      "adam: Epoch 8 Loss 0.881\n",
      "adam: Epoch 10 Loss 0.815\n",
      "adam: Epoch 12 Loss 0.940\n",
      "adam: Epoch 14 Loss 0.739\n",
      "adam: Epoch 16 Loss 0.702\n",
      "adam: Epoch 18 Loss 0.690\n",
      "adam: Epoch 20 Loss 0.671\n",
      "Training took 1.767045259475708 seconds.\n",
      "Train accuracy in percent correct: 84.65\n",
      "Test accuracy in percent correct: 81.20\n",
      "Test accuracy in percent correct: 81.20\n",
      "\n",
      "\n",
      "Patch size: [1]\n",
      "adam: Epoch 2 Loss 2.769\n",
      "adam: Epoch 4 Loss 1.830\n",
      "adam: Epoch 6 Loss 1.690\n",
      "adam: Epoch 8 Loss 1.602\n",
      "adam: Epoch 10 Loss 1.535\n",
      "adam: Epoch 12 Loss 1.487\n",
      "adam: Epoch 14 Loss 1.462\n",
      "adam: Epoch 16 Loss 1.413\n",
      "adam: Epoch 18 Loss 1.385\n",
      "adam: Epoch 20 Loss 1.362\n",
      "Training took 2.5283868312835693 seconds.\n",
      "Train accuracy in percent correct: 63.48\n",
      "Test accuracy in percent correct: 60.90\n",
      "Test accuracy in percent correct: 60.90\n",
      "\n",
      "\n",
      "Patch size: [2]\n",
      "adam: Epoch 2 Loss 2.947\n",
      "adam: Epoch 4 Loss 1.272\n",
      "adam: Epoch 6 Loss 0.891\n",
      "adam: Epoch 8 Loss 0.756\n",
      "adam: Epoch 10 Loss 0.695\n",
      "adam: Epoch 12 Loss 0.697\n",
      "adam: Epoch 14 Loss 0.579\n",
      "adam: Epoch 16 Loss 0.525\n",
      "adam: Epoch 18 Loss 0.500\n",
      "adam: Epoch 20 Loss 0.467\n",
      "Training took 1.5649724006652832 seconds.\n",
      "Train accuracy in percent correct: 89.43\n",
      "Test accuracy in percent correct: 85.02\n",
      "Test accuracy in percent correct: 85.02\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results4 = []\n",
    "n_epochs = 20\n",
    "batch_size = 500\n",
    "learning_rate = 0.01\n",
    "n_hiddens_per_conv_layer = [5]\n",
    "patch_size_per_conv_layer = [[10],[15],[16],[4],[5],[6],[12],[1],[2]]\n",
    "stride_per_conv_layer=[2]\n",
    "n_hiddens_per_fc_layer = [100]\n",
    "\n",
    "for patch in patch_size_per_conv_layer:\n",
    "    \n",
    "    print('Patch size:', patch)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cnnet = CNN2D(16 * 16, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, len(np.unique(Ttrain)), \n",
    "              patch, stride_per_conv_layer, device=device)\n",
    "    \n",
    "    cnnet.train(Xtrain, Ttrain, batch_size, n_epochs, learning_rate, method='adam')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    train_perc_correct,test_perc_correct,val_perc_correct=printstuff(Xtrain,Ttrain,Xtest,Ttest,Xval,Tval)\n",
    "    \n",
    "    experiment = [n_epochs,batch_size,learning_rate,n_hiddens_per_conv_layer, patch, stride_per_conv_layer, n_hiddens_per_fc_layer, train_perc_correct, test_perc_correct, val_perc_correct, elapsed_time]\n",
    "    results4.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Hiddens per convolutional layer</th>\n",
       "      <th>Patch size per convolutional layer</th>\n",
       "      <th>Stride per convolutional layer</th>\n",
       "      <th>Hiddens per fully-connected layer</th>\n",
       "      <th>Train percent correct</th>\n",
       "      <th>Test percent correct</th>\n",
       "      <th>Validate percent correct</th>\n",
       "      <th>Training time (seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[15]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>61.377571</td>\n",
       "      <td>60.856210</td>\n",
       "      <td>60.376272</td>\n",
       "      <td>1.589529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>63.475540</td>\n",
       "      <td>60.904204</td>\n",
       "      <td>59.694759</td>\n",
       "      <td>2.528387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[16]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>62.782603</td>\n",
       "      <td>61.864081</td>\n",
       "      <td>61.604915</td>\n",
       "      <td>1.502310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[12]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>84.653212</td>\n",
       "      <td>81.196007</td>\n",
       "      <td>81.080822</td>\n",
       "      <td>1.767045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[10]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>87.134372</td>\n",
       "      <td>82.971780</td>\n",
       "      <td>83.086965</td>\n",
       "      <td>1.565743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>89.468642</td>\n",
       "      <td>83.547706</td>\n",
       "      <td>83.816471</td>\n",
       "      <td>1.509044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>90.592668</td>\n",
       "      <td>84.229219</td>\n",
       "      <td>84.517182</td>\n",
       "      <td>1.518693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>89.906118</td>\n",
       "      <td>84.478787</td>\n",
       "      <td>84.152428</td>\n",
       "      <td>1.512728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>89.430323</td>\n",
       "      <td>85.016318</td>\n",
       "      <td>84.766750</td>\n",
       "      <td>1.564972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epochs  Batch size  Learning Rate Hiddens per convolutional layer  \\\n",
       "1      20         500           0.01                             [5]   \n",
       "7      20         500           0.01                             [5]   \n",
       "2      20         500           0.01                             [5]   \n",
       "6      20         500           0.01                             [5]   \n",
       "0      20         500           0.01                             [5]   \n",
       "4      20         500           0.01                             [5]   \n",
       "5      20         500           0.01                             [5]   \n",
       "3      20         500           0.01                             [5]   \n",
       "8      20         500           0.01                             [5]   \n",
       "\n",
       "  Patch size per convolutional layer Stride per convolutional layer  \\\n",
       "1                               [15]                            [2]   \n",
       "7                                [1]                            [2]   \n",
       "2                               [16]                            [2]   \n",
       "6                               [12]                            [2]   \n",
       "0                               [10]                            [2]   \n",
       "4                                [5]                            [2]   \n",
       "5                                [6]                            [2]   \n",
       "3                                [4]                            [2]   \n",
       "8                                [2]                            [2]   \n",
       "\n",
       "  Hiddens per fully-connected layer  Train percent correct  \\\n",
       "1                             [100]              61.377571   \n",
       "7                             [100]              63.475540   \n",
       "2                             [100]              62.782603   \n",
       "6                             [100]              84.653212   \n",
       "0                             [100]              87.134372   \n",
       "4                             [100]              89.468642   \n",
       "5                             [100]              90.592668   \n",
       "3                             [100]              89.906118   \n",
       "8                             [100]              89.430323   \n",
       "\n",
       "   Test percent correct  Validate percent correct  Training time (seconds)  \n",
       "1             60.856210                 60.376272                 1.589529  \n",
       "7             60.904204                 59.694759                 2.528387  \n",
       "2             61.864081                 61.604915                 1.502310  \n",
       "6             81.196007                 81.080822                 1.767045  \n",
       "0             82.971780                 83.086965                 1.565743  \n",
       "4             83.547706                 83.816471                 1.509044  \n",
       "5             84.229219                 84.517182                 1.518693  \n",
       "3             84.478787                 84.152428                 1.512728  \n",
       "8             85.016318                 84.766750                 1.564972  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP4 = pd.DataFrame(results4, columns=names)\n",
    "EXP4.sort_values('Test percent correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table we can see that, using a large patch size like 12 or 15 or 16 gives comparatively less accuracy values for the test set than using small patch sizes like 2 or 4. The image itself is a 16x16 image. Hence, using larger patch sizes considers the entire image at a time for computing which renders the point of convolution useless. Also, it is observed that using patch size of 1 also gives less accuracy values. The highest accuracy value for test set is observed when the patch size is kept 2. So, we go forward with keeping the patch size of the convolutional layer as [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 5: Different stride values for the convolutional layer**\n",
    "\n",
    "Now, we test with different stride values for the convolutional layer. Rest of the parameters are constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stride: [10]\n",
      "adam: Epoch 2 Loss 2.803\n",
      "adam: Epoch 4 Loss 2.731\n",
      "adam: Epoch 6 Loss 2.701\n",
      "adam: Epoch 8 Loss 2.678\n",
      "adam: Epoch 10 Loss 2.667\n",
      "adam: Epoch 12 Loss 2.661\n",
      "adam: Epoch 14 Loss 2.628\n",
      "adam: Epoch 16 Loss 2.613\n",
      "adam: Epoch 18 Loss 2.596\n",
      "adam: Epoch 20 Loss 2.586\n",
      "Training took 2.6708035469055176 seconds.\n",
      "Train accuracy in percent correct: 26.13\n",
      "Test accuracy in percent correct: 25.90\n",
      "Test accuracy in percent correct: 25.90\n",
      "\n",
      "\n",
      "Stride: [15]\n",
      "adam: Epoch 2 Loss 3.090\n",
      "adam: Epoch 4 Loss 3.062\n",
      "adam: Epoch 6 Loss 3.061\n",
      "adam: Epoch 8 Loss 3.067\n",
      "adam: Epoch 10 Loss 3.045\n",
      "adam: Epoch 12 Loss 3.043\n",
      "adam: Epoch 14 Loss 3.053\n",
      "adam: Epoch 16 Loss 3.043\n",
      "adam: Epoch 18 Loss 3.044\n",
      "adam: Epoch 20 Loss 3.033\n",
      "Training took 1.6376867294311523 seconds.\n",
      "Train accuracy in percent correct: 9.63\n",
      "Test accuracy in percent correct: 9.64\n",
      "Test accuracy in percent correct: 9.64\n",
      "\n",
      "\n",
      "Stride: [16]\n",
      "adam: Epoch 2 Loss 3.086\n",
      "adam: Epoch 4 Loss 3.058\n",
      "adam: Epoch 6 Loss 3.052\n",
      "adam: Epoch 8 Loss 3.055\n",
      "adam: Epoch 10 Loss 3.044\n",
      "adam: Epoch 12 Loss 3.035\n",
      "adam: Epoch 14 Loss 3.043\n",
      "adam: Epoch 16 Loss 3.042\n",
      "adam: Epoch 18 Loss 3.042\n",
      "adam: Epoch 20 Loss 3.026\n",
      "Training took 1.640221118927002 seconds.\n",
      "Train accuracy in percent correct: 9.63\n",
      "Test accuracy in percent correct: 9.64\n",
      "Test accuracy in percent correct: 9.64\n",
      "\n",
      "\n",
      "Stride: [4]\n",
      "adam: Epoch 2 Loss 2.218\n",
      "adam: Epoch 4 Loss 2.017\n",
      "adam: Epoch 6 Loss 1.951\n",
      "adam: Epoch 8 Loss 1.900\n",
      "adam: Epoch 10 Loss 1.857\n",
      "adam: Epoch 12 Loss 1.819\n",
      "adam: Epoch 14 Loss 1.784\n",
      "adam: Epoch 16 Loss 1.753\n",
      "adam: Epoch 18 Loss 1.723\n",
      "adam: Epoch 20 Loss 1.696\n",
      "Training took 1.7064580917358398 seconds.\n",
      "Train accuracy in percent correct: 52.79\n",
      "Test accuracy in percent correct: 52.21\n",
      "Test accuracy in percent correct: 52.21\n",
      "\n",
      "\n",
      "Stride: [5]\n",
      "adam: Epoch 2 Loss 2.362\n",
      "adam: Epoch 4 Loss 2.203\n",
      "adam: Epoch 6 Loss 2.143\n",
      "adam: Epoch 8 Loss 2.100\n",
      "adam: Epoch 10 Loss 2.061\n",
      "adam: Epoch 12 Loss 2.023\n",
      "adam: Epoch 14 Loss 1.987\n",
      "adam: Epoch 16 Loss 1.952\n",
      "adam: Epoch 18 Loss 1.917\n",
      "adam: Epoch 20 Loss 1.885\n",
      "Training took 1.5928599834442139 seconds.\n",
      "Train accuracy in percent correct: 46.65\n",
      "Test accuracy in percent correct: 46.47\n",
      "Test accuracy in percent correct: 46.47\n",
      "\n",
      "\n",
      "Stride: [6]\n",
      "adam: Epoch 2 Loss 2.791\n",
      "adam: Epoch 4 Loss 2.678\n",
      "adam: Epoch 6 Loss 2.609\n",
      "adam: Epoch 8 Loss 2.568\n",
      "adam: Epoch 10 Loss 2.543\n",
      "adam: Epoch 12 Loss 2.525\n",
      "adam: Epoch 14 Loss 2.510\n",
      "adam: Epoch 16 Loss 2.498\n",
      "adam: Epoch 18 Loss 2.493\n",
      "adam: Epoch 20 Loss 2.490\n",
      "Training took 1.5954103469848633 seconds.\n",
      "Train accuracy in percent correct: 28.90\n",
      "Test accuracy in percent correct: 28.05\n",
      "Test accuracy in percent correct: 28.05\n",
      "\n",
      "\n",
      "Stride: [12]\n",
      "adam: Epoch 2 Loss 3.083\n",
      "adam: Epoch 4 Loss 3.051\n",
      "adam: Epoch 6 Loss 3.041\n",
      "adam: Epoch 8 Loss 3.039\n",
      "adam: Epoch 10 Loss 3.035\n",
      "adam: Epoch 12 Loss 3.031\n",
      "adam: Epoch 14 Loss 3.029\n",
      "adam: Epoch 16 Loss 3.027\n",
      "adam: Epoch 18 Loss 3.267\n",
      "adam: Epoch 20 Loss 6.607\n",
      "Training took 1.6114542484283447 seconds.\n",
      "Train accuracy in percent correct: 9.63\n",
      "Test accuracy in percent correct: 9.64\n",
      "Test accuracy in percent correct: 9.64\n",
      "\n",
      "\n",
      "Stride: [1]\n",
      "adam: Epoch 2 Loss 4.117\n",
      "adam: Epoch 4 Loss 4.158\n",
      "adam: Epoch 6 Loss 4.165\n",
      "adam: Epoch 8 Loss 4.169\n",
      "adam: Epoch 10 Loss 4.170\n",
      "adam: Epoch 12 Loss 4.172\n",
      "adam: Epoch 14 Loss 4.173\n",
      "adam: Epoch 16 Loss 4.173\n",
      "adam: Epoch 18 Loss 4.174\n",
      "adam: Epoch 20 Loss 4.174\n",
      "Training took 2.064680576324463 seconds.\n",
      "Train accuracy in percent correct: 9.63\n",
      "Test accuracy in percent correct: 9.64\n",
      "Test accuracy in percent correct: 9.64\n",
      "\n",
      "\n",
      "Stride: [2]\n",
      "adam: Epoch 2 Loss 1.627\n",
      "adam: Epoch 4 Loss 1.016\n",
      "adam: Epoch 6 Loss 0.844\n",
      "adam: Epoch 8 Loss 0.727\n",
      "adam: Epoch 10 Loss 0.649\n",
      "adam: Epoch 12 Loss 0.598\n",
      "adam: Epoch 14 Loss 0.533\n",
      "adam: Epoch 16 Loss 0.490\n",
      "adam: Epoch 18 Loss 0.452\n",
      "adam: Epoch 20 Loss 0.422\n",
      "Training took 2.5621209144592285 seconds.\n",
      "Train accuracy in percent correct: 90.30\n",
      "Test accuracy in percent correct: 85.43\n",
      "Test accuracy in percent correct: 85.43\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results5 = []\n",
    "n_epochs = 20\n",
    "batch_size = 500\n",
    "learning_rate = 0.01\n",
    "n_hiddens_per_conv_layer = [5]\n",
    "patch_size_per_conv_layer = [2]\n",
    "stride_per_conv_layer = [[10],[15],[16],[4],[5],[6],[12],[1],[2]]\n",
    "n_hiddens_per_fc_layer = [100]\n",
    "\n",
    "for stride in stride_per_conv_layer:\n",
    "    \n",
    "    print('Stride:', stride)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cnnet = CNN2D(16 * 16, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, len(np.unique(Ttrain)), \n",
    "              patch_size_per_conv_layer, stride, device=device)\n",
    "    \n",
    "    cnnet.train(Xtrain, Ttrain, batch_size, n_epochs, learning_rate, method='adam')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    train_perc_correct,test_perc_correct,val_perc_correct=printstuff(Xtrain,Ttrain,Xtest,Ttest,Xval,Tval)\n",
    "    \n",
    "    experiment = [n_epochs,batch_size,learning_rate,n_hiddens_per_conv_layer, patch_size_per_conv_layer, stride, n_hiddens_per_fc_layer, train_perc_correct, test_perc_correct, val_perc_correct, elapsed_time]\n",
    "    results5.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Hiddens per convolutional layer</th>\n",
       "      <th>Patch size per convolutional layer</th>\n",
       "      <th>Stride per convolutional layer</th>\n",
       "      <th>Hiddens per fully-connected layer</th>\n",
       "      <th>Train percent correct</th>\n",
       "      <th>Test percent correct</th>\n",
       "      <th>Validate percent correct</th>\n",
       "      <th>Training time (seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[15]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>9.630860</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>1.637687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[16]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>9.630860</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>1.640221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[12]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>9.630860</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>1.611454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>9.630860</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>9.637166</td>\n",
       "      <td>2.064681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[10]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>26.127219</td>\n",
       "      <td>25.897485</td>\n",
       "      <td>26.540603</td>\n",
       "      <td>2.670804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>28.895772</td>\n",
       "      <td>28.047610</td>\n",
       "      <td>28.325974</td>\n",
       "      <td>1.595410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>46.653468</td>\n",
       "      <td>46.467652</td>\n",
       "      <td>45.651757</td>\n",
       "      <td>1.592860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>52.790906</td>\n",
       "      <td>52.207717</td>\n",
       "      <td>51.123056</td>\n",
       "      <td>1.706458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>90.295695</td>\n",
       "      <td>85.429065</td>\n",
       "      <td>85.582645</td>\n",
       "      <td>2.562121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epochs  Batch size  Learning Rate Hiddens per convolutional layer  \\\n",
       "1      20         500           0.01                             [5]   \n",
       "2      20         500           0.01                             [5]   \n",
       "6      20         500           0.01                             [5]   \n",
       "7      20         500           0.01                             [5]   \n",
       "0      20         500           0.01                             [5]   \n",
       "5      20         500           0.01                             [5]   \n",
       "4      20         500           0.01                             [5]   \n",
       "3      20         500           0.01                             [5]   \n",
       "8      20         500           0.01                             [5]   \n",
       "\n",
       "  Patch size per convolutional layer Stride per convolutional layer  \\\n",
       "1                                [2]                           [15]   \n",
       "2                                [2]                           [16]   \n",
       "6                                [2]                           [12]   \n",
       "7                                [2]                            [1]   \n",
       "0                                [2]                           [10]   \n",
       "5                                [2]                            [6]   \n",
       "4                                [2]                            [5]   \n",
       "3                                [2]                            [4]   \n",
       "8                                [2]                            [2]   \n",
       "\n",
       "  Hiddens per fully-connected layer  Train percent correct  \\\n",
       "1                             [100]               9.630860   \n",
       "2                             [100]               9.630860   \n",
       "6                             [100]               9.630860   \n",
       "7                             [100]               9.630860   \n",
       "0                             [100]              26.127219   \n",
       "5                             [100]              28.895772   \n",
       "4                             [100]              46.653468   \n",
       "3                             [100]              52.790906   \n",
       "8                             [100]              90.295695   \n",
       "\n",
       "   Test percent correct  Validate percent correct  Training time (seconds)  \n",
       "1              9.637166                  9.637166                 1.637687  \n",
       "2              9.637166                  9.637166                 1.640221  \n",
       "6              9.637166                  9.637166                 1.611454  \n",
       "7              9.637166                  9.637166                 2.064681  \n",
       "0             25.897485                 26.540603                 2.670804  \n",
       "5             28.047610                 28.325974                 1.595410  \n",
       "4             46.467652                 45.651757                 1.592860  \n",
       "3             52.207717                 51.123056                 1.706458  \n",
       "8             85.429065                 85.582645                 2.562121  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP5 = pd.DataFrame(results5, columns=names)\n",
    "EXP5.sort_values('Test percent correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table we can see that, using a stride values like 12 or 15 or 16 gives very less accuracy values of 9% for the test set. Even using stride value of 1 gives low accuracy of 9.63%. The highest accuracy value for test set is observed when the stride value is kept 2. So, we go forward with keeping the stride value of the convolutional layer as [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 6: Different learning rate values**\n",
    "\n",
    "Now, we test with different learning rates. Rest of the parameters are constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.1\n",
      "adam: Epoch 2 Loss 17.710\n",
      "adam: Epoch 4 Loss 17.605\n",
      "adam: Epoch 6 Loss 17.837\n",
      "adam: Epoch 8 Loss 18.435\n",
      "adam: Epoch 10 Loss 17.735\n",
      "adam: Epoch 12 Loss 18.078\n",
      "adam: Epoch 14 Loss 18.329\n",
      "adam: Epoch 16 Loss 17.544\n",
      "adam: Epoch 18 Loss 18.083\n",
      "adam: Epoch 20 Loss 18.447\n",
      "Training took 3.083775043487549 seconds.\n",
      "Train accuracy in percent correct: 9.42\n",
      "Test accuracy in percent correct: 9.43\n",
      "Test accuracy in percent correct: 9.43\n",
      "\n",
      "\n",
      "Learning Rate: 0.01\n",
      "adam: Epoch 2 Loss 1.684\n",
      "adam: Epoch 4 Loss 1.045\n",
      "adam: Epoch 6 Loss 0.840\n",
      "adam: Epoch 8 Loss 0.760\n",
      "adam: Epoch 10 Loss 0.645\n",
      "adam: Epoch 12 Loss 0.600\n",
      "adam: Epoch 14 Loss 0.536\n",
      "adam: Epoch 16 Loss 0.487\n",
      "adam: Epoch 18 Loss 0.460\n",
      "adam: Epoch 20 Loss 0.415\n",
      "Training took 1.804875373840332 seconds.\n",
      "Train accuracy in percent correct: 90.26\n",
      "Test accuracy in percent correct: 85.46\n",
      "Test accuracy in percent correct: 85.46\n",
      "\n",
      "\n",
      "Learning Rate: 0.001\n",
      "adam: Epoch 2 Loss 2.352\n",
      "adam: Epoch 4 Loss 1.618\n",
      "adam: Epoch 6 Loss 1.301\n",
      "adam: Epoch 8 Loss 1.143\n",
      "adam: Epoch 10 Loss 1.044\n",
      "adam: Epoch 12 Loss 0.973\n",
      "adam: Epoch 14 Loss 0.918\n",
      "adam: Epoch 16 Loss 0.872\n",
      "adam: Epoch 18 Loss 0.833\n",
      "adam: Epoch 20 Loss 0.799\n",
      "Training took 1.6426827907562256 seconds.\n",
      "Train accuracy in percent correct: 79.52\n",
      "Test accuracy in percent correct: 78.33\n",
      "Test accuracy in percent correct: 78.33\n",
      "\n",
      "\n",
      "Learning Rate: 0.03\n",
      "adam: Epoch 2 Loss 2.279\n",
      "adam: Epoch 4 Loss 2.030\n",
      "adam: Epoch 6 Loss 1.140\n",
      "adam: Epoch 8 Loss 0.811\n",
      "adam: Epoch 10 Loss 0.784\n",
      "adam: Epoch 12 Loss 0.794\n",
      "adam: Epoch 14 Loss 0.618\n",
      "adam: Epoch 16 Loss 0.881\n",
      "adam: Epoch 18 Loss 0.673\n",
      "adam: Epoch 20 Loss 0.507\n",
      "Training took 1.631749153137207 seconds.\n",
      "Train accuracy in percent correct: 89.32\n",
      "Test accuracy in percent correct: 82.87\n",
      "Test accuracy in percent correct: 82.87\n",
      "\n",
      "\n",
      "Learning Rate: 0.05\n",
      "adam: Epoch 2 Loss 10.185\n",
      "adam: Epoch 4 Loss 10.493\n",
      "adam: Epoch 6 Loss 10.027\n",
      "adam: Epoch 8 Loss 10.344\n",
      "adam: Epoch 10 Loss 10.324\n",
      "adam: Epoch 12 Loss 10.084\n",
      "adam: Epoch 14 Loss 10.317\n",
      "adam: Epoch 16 Loss 10.427\n",
      "adam: Epoch 18 Loss 10.538\n",
      "adam: Epoch 20 Loss 10.143\n",
      "Training took 1.6297049522399902 seconds.\n",
      "Train accuracy in percent correct: 4.09\n",
      "Test accuracy in percent correct: 4.10\n",
      "Test accuracy in percent correct: 4.10\n",
      "\n",
      "\n",
      "Learning Rate: 0.08\n",
      "adam: Epoch 2 Loss 14.523\n",
      "adam: Epoch 4 Loss 12.851\n",
      "adam: Epoch 6 Loss 7.143\n",
      "adam: Epoch 8 Loss 3.692\n",
      "adam: Epoch 10 Loss 3.018\n",
      "adam: Epoch 12 Loss 2.504\n",
      "adam: Epoch 14 Loss 2.313\n",
      "adam: Epoch 16 Loss 1.808\n",
      "adam: Epoch 18 Loss 1.532\n",
      "adam: Epoch 20 Loss 1.828\n",
      "Training took 1.6262538433074951 seconds.\n",
      "Train accuracy in percent correct: 75.23\n",
      "Test accuracy in percent correct: 74.22\n",
      "Test accuracy in percent correct: 74.22\n",
      "\n",
      "\n",
      "Learning Rate: 0.2\n",
      "adam: Epoch 2 Loss 36.657\n",
      "adam: Epoch 4 Loss 37.027\n",
      "adam: Epoch 6 Loss 35.070\n",
      "adam: Epoch 8 Loss 35.955\n",
      "adam: Epoch 10 Loss 33.976\n",
      "adam: Epoch 12 Loss 33.897\n",
      "adam: Epoch 14 Loss 33.282\n",
      "adam: Epoch 16 Loss 37.334\n",
      "adam: Epoch 18 Loss 35.942\n",
      "adam: Epoch 20 Loss 33.173\n",
      "Training took 1.9435205459594727 seconds.\n",
      "Train accuracy in percent correct: 5.13\n",
      "Test accuracy in percent correct: 5.13\n",
      "Test accuracy in percent correct: 5.13\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results6 = []\n",
    "n_epochs = 20\n",
    "batch_size = 500\n",
    "learning_rate = [0.1,0.01,0.001,0.03,0.05,0.08, 0.2]\n",
    "n_hiddens_per_conv_layer = [5]\n",
    "patch_size_per_conv_layer = [2]\n",
    "stride_per_conv_layer = [2]\n",
    "n_hiddens_per_fc_layer = [100]\n",
    "\n",
    "for lr in learning_rate:\n",
    "    \n",
    "    print('Learning Rate:', lr)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cnnet = CNN2D(16 * 16, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, len(np.unique(Ttrain)), \n",
    "              patch_size_per_conv_layer, stride_per_conv_layer, device=device)\n",
    "    \n",
    "    cnnet.train(Xtrain, Ttrain, batch_size, n_epochs, lr, method='adam')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    train_perc_correct,test_perc_correct,val_perc_correct=printstuff(Xtrain,Ttrain,Xtest,Ttest,Xval,Tval)\n",
    "    \n",
    "    experiment = [n_epochs,batch_size,lr,n_hiddens_per_conv_layer, patch_size_per_conv_layer, stride_per_conv_layer, n_hiddens_per_fc_layer, train_perc_correct, test_perc_correct, val_perc_correct, elapsed_time]\n",
    "    results6.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Hiddens per convolutional layer</th>\n",
       "      <th>Patch size per convolutional layer</th>\n",
       "      <th>Stride per convolutional layer</th>\n",
       "      <th>Hiddens per fully-connected layer</th>\n",
       "      <th>Train percent correct</th>\n",
       "      <th>Test percent correct</th>\n",
       "      <th>Validate percent correct</th>\n",
       "      <th>Training time (seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.050</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>4.093754</td>\n",
       "      <td>4.098675</td>\n",
       "      <td>4.098675</td>\n",
       "      <td>1.629705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.200</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>5.125176</td>\n",
       "      <td>5.125744</td>\n",
       "      <td>5.125744</td>\n",
       "      <td>1.943521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.100</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>9.416911</td>\n",
       "      <td>9.425993</td>\n",
       "      <td>9.425993</td>\n",
       "      <td>3.083775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.080</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>75.233108</td>\n",
       "      <td>74.217700</td>\n",
       "      <td>73.190632</td>\n",
       "      <td>1.626254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.001</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>79.518457</td>\n",
       "      <td>78.325974</td>\n",
       "      <td>78.229987</td>\n",
       "      <td>1.642683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.030</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>89.321752</td>\n",
       "      <td>82.866193</td>\n",
       "      <td>83.269342</td>\n",
       "      <td>1.631749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.010</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>90.263763</td>\n",
       "      <td>85.457861</td>\n",
       "      <td>85.429065</td>\n",
       "      <td>1.804875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epochs  Batch size  Learning Rate Hiddens per convolutional layer  \\\n",
       "4      20         500          0.050                             [5]   \n",
       "6      20         500          0.200                             [5]   \n",
       "0      20         500          0.100                             [5]   \n",
       "5      20         500          0.080                             [5]   \n",
       "2      20         500          0.001                             [5]   \n",
       "3      20         500          0.030                             [5]   \n",
       "1      20         500          0.010                             [5]   \n",
       "\n",
       "  Patch size per convolutional layer Stride per convolutional layer  \\\n",
       "4                                [2]                            [2]   \n",
       "6                                [2]                            [2]   \n",
       "0                                [2]                            [2]   \n",
       "5                                [2]                            [2]   \n",
       "2                                [2]                            [2]   \n",
       "3                                [2]                            [2]   \n",
       "1                                [2]                            [2]   \n",
       "\n",
       "  Hiddens per fully-connected layer  Train percent correct  \\\n",
       "4                             [100]               4.093754   \n",
       "6                             [100]               5.125176   \n",
       "0                             [100]               9.416911   \n",
       "5                             [100]              75.233108   \n",
       "2                             [100]              79.518457   \n",
       "3                             [100]              89.321752   \n",
       "1                             [100]              90.263763   \n",
       "\n",
       "   Test percent correct  Validate percent correct  Training time (seconds)  \n",
       "4              4.098675                  4.098675                 1.629705  \n",
       "6              5.125744                  5.125744                 1.943521  \n",
       "0              9.425993                  9.425993                 3.083775  \n",
       "5             74.217700                 73.190632                 1.626254  \n",
       "2             78.325974                 78.229987                 1.642683  \n",
       "3             82.866193                 83.269342                 1.631749  \n",
       "1             85.457861                 85.429065                 1.804875  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP6 = pd.DataFrame(results6, columns=names)\n",
    "EXP6.sort_values('Test percent correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table we can see that, for learning rate values 0.2, 0.1 and 0.05, accuracy values for the test set are way too less. This can be because these learning rates are high and so, the value obtained from the function might be jumping either left or right of the minimum value. Using 0.001 as learning rate might require more number of epochs to train the model. The highest accuracy value for test set is observed when the learning rate is kept 0.01. So, we go forward with keeping the patch size of the convolutional layer as 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 7: Different batch sizes**\n",
    "\n",
    "Now, we test with different batch sizes. Rest of the parameters are constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 10\n",
      "adam: Epoch 2 Loss 2.389\n",
      "adam: Epoch 4 Loss 2.316\n",
      "adam: Epoch 6 Loss 1.626\n",
      "adam: Epoch 8 Loss 1.712\n",
      "adam: Epoch 10 Loss 1.587\n",
      "adam: Epoch 12 Loss 1.806\n",
      "adam: Epoch 14 Loss 1.689\n",
      "adam: Epoch 16 Loss 1.808\n",
      "adam: Epoch 18 Loss 1.826\n",
      "adam: Epoch 20 Loss 1.842\n",
      "Training took 149.6403148174286 seconds.\n",
      "Train accuracy in percent correct: 14.88\n",
      "Test accuracy in percent correct: 14.93\n",
      "Test accuracy in percent correct: 14.93\n",
      "\n",
      "\n",
      "Batch size: 50\n",
      "adam: Epoch 2 Loss 2.633\n",
      "adam: Epoch 4 Loss 2.430\n",
      "adam: Epoch 6 Loss 2.272\n",
      "adam: Epoch 8 Loss 2.325\n",
      "adam: Epoch 10 Loss 2.156\n",
      "adam: Epoch 12 Loss 1.865\n",
      "adam: Epoch 14 Loss 1.962\n",
      "adam: Epoch 16 Loss 1.901\n",
      "adam: Epoch 18 Loss 1.719\n",
      "adam: Epoch 20 Loss 1.787\n",
      "Training took 21.348029851913452 seconds.\n",
      "Train accuracy in percent correct: 53.44\n",
      "Test accuracy in percent correct: 52.73\n",
      "Test accuracy in percent correct: 52.73\n",
      "\n",
      "\n",
      "Batch size: 100\n",
      "adam: Epoch 2 Loss 3.847\n",
      "adam: Epoch 4 Loss 2.490\n",
      "adam: Epoch 6 Loss 1.688\n",
      "adam: Epoch 8 Loss 1.671\n",
      "adam: Epoch 10 Loss 1.680\n",
      "adam: Epoch 12 Loss 1.586\n",
      "adam: Epoch 14 Loss 1.492\n",
      "adam: Epoch 16 Loss 1.424\n",
      "adam: Epoch 18 Loss 1.184\n",
      "adam: Epoch 20 Loss 1.098\n",
      "Training took 7.55597186088562 seconds.\n",
      "Train accuracy in percent correct: 76.91\n",
      "Test accuracy in percent correct: 74.62\n",
      "Test accuracy in percent correct: 74.62\n",
      "\n",
      "\n",
      "Batch size: 500\n",
      "adam: Epoch 2 Loss 1.807\n",
      "adam: Epoch 4 Loss 1.061\n",
      "adam: Epoch 6 Loss 0.818\n",
      "adam: Epoch 8 Loss 0.752\n",
      "adam: Epoch 10 Loss 0.635\n",
      "adam: Epoch 12 Loss 0.584\n",
      "adam: Epoch 14 Loss 0.547\n",
      "adam: Epoch 16 Loss 0.485\n",
      "adam: Epoch 18 Loss 0.456\n",
      "adam: Epoch 20 Loss 0.426\n",
      "Training took 1.6015911102294922 seconds.\n",
      "Train accuracy in percent correct: 90.58\n",
      "Test accuracy in percent correct: 85.14\n",
      "Test accuracy in percent correct: 85.14\n",
      "\n",
      "\n",
      "Batch size: 600\n",
      "adam: Epoch 2 Loss 1.872\n",
      "adam: Epoch 4 Loss 1.108\n",
      "adam: Epoch 6 Loss 0.817\n",
      "adam: Epoch 8 Loss 0.713\n",
      "adam: Epoch 10 Loss 0.694\n",
      "adam: Epoch 12 Loss 0.583\n",
      "adam: Epoch 14 Loss 0.528\n",
      "adam: Epoch 16 Loss 0.498\n",
      "adam: Epoch 18 Loss 0.465\n",
      "adam: Epoch 20 Loss 0.432\n",
      "Training took 1.3359146118164062 seconds.\n",
      "Train accuracy in percent correct: 89.62\n",
      "Test accuracy in percent correct: 85.13\n",
      "Test accuracy in percent correct: 85.13\n",
      "\n",
      "\n",
      "Batch size: 450\n",
      "adam: Epoch 2 Loss 2.699\n",
      "adam: Epoch 4 Loss 1.267\n",
      "adam: Epoch 6 Loss 0.912\n",
      "adam: Epoch 8 Loss 0.783\n",
      "adam: Epoch 10 Loss 0.709\n",
      "adam: Epoch 12 Loss 0.626\n",
      "adam: Epoch 14 Loss 0.586\n",
      "adam: Epoch 16 Loss 0.544\n",
      "adam: Epoch 18 Loss 0.494\n",
      "adam: Epoch 20 Loss 0.463\n",
      "Training took 1.747148036956787 seconds.\n",
      "Train accuracy in percent correct: 89.21\n",
      "Test accuracy in percent correct: 84.94\n",
      "Test accuracy in percent correct: 84.94\n",
      "\n",
      "\n",
      "Batch size: 550\n",
      "adam: Epoch 2 Loss 1.737\n",
      "adam: Epoch 4 Loss 1.092\n",
      "adam: Epoch 6 Loss 0.825\n",
      "adam: Epoch 8 Loss 0.741\n",
      "adam: Epoch 10 Loss 0.654\n",
      "adam: Epoch 12 Loss 0.605\n",
      "adam: Epoch 14 Loss 0.545\n",
      "adam: Epoch 16 Loss 0.507\n",
      "adam: Epoch 18 Loss 0.464\n",
      "adam: Epoch 20 Loss 0.435\n",
      "Training took 1.7381401062011719 seconds.\n",
      "Train accuracy in percent correct: 89.58\n",
      "Test accuracy in percent correct: 84.90\n",
      "Test accuracy in percent correct: 84.90\n",
      "\n",
      "\n",
      "Batch size: 1000\n",
      "adam: Epoch 2 Loss 1.727\n",
      "adam: Epoch 4 Loss 0.971\n",
      "adam: Epoch 6 Loss 0.807\n",
      "adam: Epoch 8 Loss 0.734\n",
      "adam: Epoch 10 Loss 0.646\n",
      "adam: Epoch 12 Loss 0.591\n",
      "adam: Epoch 14 Loss 0.546\n",
      "adam: Epoch 16 Loss 0.508\n",
      "adam: Epoch 18 Loss 0.477\n",
      "adam: Epoch 20 Loss 0.478\n",
      "Training took 0.8495070934295654 seconds.\n",
      "Train accuracy in percent correct: 88.25\n",
      "Test accuracy in percent correct: 84.89\n",
      "Test accuracy in percent correct: 84.89\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results7 = []\n",
    "n_epochs = 20\n",
    "batch_size = [10,50,100,500,600,450,550,1000]\n",
    "learning_rate = 0.01\n",
    "n_hiddens_per_conv_layer = [5]\n",
    "patch_size_per_conv_layer = [2]\n",
    "stride_per_conv_layer = [2]\n",
    "n_hiddens_per_fc_layer = [100]\n",
    "\n",
    "for bs in batch_size:\n",
    "    \n",
    "    print('Batch size:', bs)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cnnet = CNN2D(16 * 16, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, len(np.unique(Ttrain)), \n",
    "              patch_size_per_conv_layer, stride_per_conv_layer, device=device)\n",
    "    \n",
    "    cnnet.train(Xtrain, Ttrain, bs, n_epochs, learning_rate, method='adam')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    train_perc_correct,test_perc_correct,val_perc_correct=printstuff(Xtrain,Ttrain,Xtest,Ttest,Xval,Tval)\n",
    "    \n",
    "    experiment = [n_epochs,bs,learning_rate,n_hiddens_per_conv_layer, patch_size_per_conv_layer, stride_per_conv_layer, n_hiddens_per_fc_layer, train_perc_correct, test_perc_correct, val_perc_correct, elapsed_time]\n",
    "    results7.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Hiddens per convolutional layer</th>\n",
       "      <th>Patch size per convolutional layer</th>\n",
       "      <th>Stride per convolutional layer</th>\n",
       "      <th>Hiddens per fully-connected layer</th>\n",
       "      <th>Train percent correct</th>\n",
       "      <th>Test percent correct</th>\n",
       "      <th>Validate percent correct</th>\n",
       "      <th>Training time (seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>14.877379</td>\n",
       "      <td>14.926089</td>\n",
       "      <td>14.570935</td>\n",
       "      <td>149.640315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>53.442330</td>\n",
       "      <td>52.726051</td>\n",
       "      <td>52.927625</td>\n",
       "      <td>21.348030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>76.906374</td>\n",
       "      <td>74.620849</td>\n",
       "      <td>73.920138</td>\n",
       "      <td>7.555972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>88.245625</td>\n",
       "      <td>84.891534</td>\n",
       "      <td>84.306009</td>\n",
       "      <td>0.849507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>550</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>89.583599</td>\n",
       "      <td>84.901133</td>\n",
       "      <td>84.584373</td>\n",
       "      <td>1.738140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>450</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>89.213182</td>\n",
       "      <td>84.939528</td>\n",
       "      <td>84.689960</td>\n",
       "      <td>1.747148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>600</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>89.615532</td>\n",
       "      <td>85.131503</td>\n",
       "      <td>85.477059</td>\n",
       "      <td>1.335915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>90.576702</td>\n",
       "      <td>85.141102</td>\n",
       "      <td>85.208293</td>\n",
       "      <td>1.601591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epochs  Batch size  Learning Rate Hiddens per convolutional layer  \\\n",
       "0      20          10           0.01                             [5]   \n",
       "1      20          50           0.01                             [5]   \n",
       "2      20         100           0.01                             [5]   \n",
       "7      20        1000           0.01                             [5]   \n",
       "6      20         550           0.01                             [5]   \n",
       "5      20         450           0.01                             [5]   \n",
       "4      20         600           0.01                             [5]   \n",
       "3      20         500           0.01                             [5]   \n",
       "\n",
       "  Patch size per convolutional layer Stride per convolutional layer  \\\n",
       "0                                [2]                            [2]   \n",
       "1                                [2]                            [2]   \n",
       "2                                [2]                            [2]   \n",
       "7                                [2]                            [2]   \n",
       "6                                [2]                            [2]   \n",
       "5                                [2]                            [2]   \n",
       "4                                [2]                            [2]   \n",
       "3                                [2]                            [2]   \n",
       "\n",
       "  Hiddens per fully-connected layer  Train percent correct  \\\n",
       "0                             [100]              14.877379   \n",
       "1                             [100]              53.442330   \n",
       "2                             [100]              76.906374   \n",
       "7                             [100]              88.245625   \n",
       "6                             [100]              89.583599   \n",
       "5                             [100]              89.213182   \n",
       "4                             [100]              89.615532   \n",
       "3                             [100]              90.576702   \n",
       "\n",
       "   Test percent correct  Validate percent correct  Training time (seconds)  \n",
       "0             14.926089                 14.570935               149.640315  \n",
       "1             52.726051                 52.927625                21.348030  \n",
       "2             74.620849                 73.920138                 7.555972  \n",
       "7             84.891534                 84.306009                 0.849507  \n",
       "6             84.901133                 84.584373                 1.738140  \n",
       "5             84.939528                 84.689960                 1.747148  \n",
       "4             85.131503                 85.477059                 1.335915  \n",
       "3             85.141102                 85.208293                 1.601591  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP7 = pd.DataFrame(results7, columns=names)\n",
    "EXP7.sort_values('Test percent correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table we can see that, keeping the batch size 10 takes training time and also gives less accuracy values for the test set. Keeping batch size 1000 gives the fastest training time while giving high accuracy values. Increasing batch size increases the graphic memory usage and therefore require more GPU memory for computation. Batch size from 500 to 1000 give very high and very similar accuracy values for the test set. The highest accuracy value is observed when the batch size is kept 500. So, we go forward with keeping the batch size as 500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment 8: Different epoch values**\n",
    "\n",
    "Finally, we test with different values for number of epochs. Rest of the parameters are constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10\n",
      "adam: Epoch 1 Loss 4.175\n",
      "adam: Epoch 2 Loss 1.646\n",
      "adam: Epoch 3 Loss 1.311\n",
      "adam: Epoch 4 Loss 1.056\n",
      "adam: Epoch 5 Loss 0.881\n",
      "adam: Epoch 6 Loss 0.805\n",
      "adam: Epoch 7 Loss 0.764\n",
      "adam: Epoch 8 Loss 0.782\n",
      "adam: Epoch 9 Loss 0.718\n",
      "adam: Epoch 10 Loss 0.642\n",
      "Training took 1.0426199436187744 seconds.\n",
      "Train accuracy in percent correct: 85.25\n",
      "Test accuracy in percent correct: 83.11\n",
      "Test accuracy in percent correct: 83.11\n",
      "\n",
      "\n",
      "Epochs: 20\n",
      "adam: Epoch 2 Loss 1.589\n",
      "adam: Epoch 4 Loss 1.033\n",
      "adam: Epoch 6 Loss 0.828\n",
      "adam: Epoch 8 Loss 0.739\n",
      "adam: Epoch 10 Loss 0.633\n",
      "adam: Epoch 12 Loss 0.577\n",
      "adam: Epoch 14 Loss 0.521\n",
      "adam: Epoch 16 Loss 0.474\n",
      "adam: Epoch 18 Loss 0.441\n",
      "adam: Epoch 20 Loss 0.409\n",
      "Training took 1.7783966064453125 seconds.\n",
      "Train accuracy in percent correct: 90.47\n",
      "Test accuracy in percent correct: 85.79\n",
      "Test accuracy in percent correct: 85.79\n",
      "\n",
      "\n",
      "Epochs: 50\n",
      "adam: Epoch 5 Loss 0.902\n",
      "adam: Epoch 10 Loss 0.644\n",
      "adam: Epoch 15 Loss 0.529\n",
      "adam: Epoch 20 Loss 0.423\n",
      "adam: Epoch 25 Loss 0.361\n",
      "adam: Epoch 30 Loss 0.312\n",
      "adam: Epoch 35 Loss 0.275\n",
      "adam: Epoch 40 Loss 0.270\n",
      "adam: Epoch 45 Loss 0.286\n",
      "adam: Epoch 50 Loss 0.194\n",
      "Training took 7.629286050796509 seconds.\n",
      "Train accuracy in percent correct: 95.86\n",
      "Test accuracy in percent correct: 85.55\n",
      "Test accuracy in percent correct: 85.55\n",
      "\n",
      "\n",
      "Epochs: 70\n",
      "adam: Epoch 7 Loss 0.771\n",
      "adam: Epoch 14 Loss 0.546\n",
      "adam: Epoch 21 Loss 0.400\n",
      "adam: Epoch 28 Loss 0.316\n",
      "adam: Epoch 35 Loss 0.260\n",
      "adam: Epoch 42 Loss 0.217\n",
      "adam: Epoch 49 Loss 0.208\n",
      "adam: Epoch 56 Loss 0.172\n",
      "adam: Epoch 63 Loss 0.159\n",
      "adam: Epoch 70 Loss 0.160\n",
      "Training took 6.5287346839904785 seconds.\n",
      "Train accuracy in percent correct: 95.65\n",
      "Test accuracy in percent correct: 84.80\n",
      "Test accuracy in percent correct: 84.80\n",
      "\n",
      "\n",
      "Epochs: 100\n",
      "adam: Epoch 10 Loss 0.670\n",
      "adam: Epoch 20 Loss 0.446\n",
      "adam: Epoch 30 Loss 0.324\n",
      "adam: Epoch 40 Loss 0.252\n",
      "adam: Epoch 50 Loss 0.311\n",
      "adam: Epoch 60 Loss 0.159\n",
      "adam: Epoch 70 Loss 0.198\n",
      "adam: Epoch 80 Loss 0.130\n",
      "adam: Epoch 90 Loss 0.099\n",
      "adam: Epoch 100 Loss 0.082\n",
      "Training took 15.161429643630981 seconds.\n",
      "Train accuracy in percent correct: 97.87\n",
      "Test accuracy in percent correct: 83.97\n",
      "Test accuracy in percent correct: 83.97\n",
      "\n",
      "\n",
      "Epochs: 500\n",
      "adam: Epoch 50 Loss 0.312\n",
      "adam: Epoch 100 Loss 0.098\n",
      "adam: Epoch 150 Loss 0.085\n",
      "adam: Epoch 200 Loss 0.074\n",
      "adam: Epoch 250 Loss 0.041\n",
      "adam: Epoch 300 Loss 0.033\n",
      "adam: Epoch 350 Loss 0.028\n",
      "adam: Epoch 400 Loss 0.034\n",
      "adam: Epoch 450 Loss 0.106\n",
      "adam: Epoch 500 Loss 0.026\n",
      "Training took 69.35911917686462 seconds.\n",
      "Train accuracy in percent correct: 98.92\n",
      "Test accuracy in percent correct: 83.43\n",
      "Test accuracy in percent correct: 83.43\n",
      "\n",
      "\n",
      "Epochs: 1000\n",
      "adam: Epoch 100 Loss 0.091\n",
      "adam: Epoch 200 Loss 0.058\n",
      "adam: Epoch 300 Loss 0.040\n",
      "adam: Epoch 400 Loss 0.027\n",
      "adam: Epoch 500 Loss 0.027\n",
      "adam: Epoch 600 Loss 0.026\n",
      "adam: Epoch 700 Loss 0.438\n",
      "adam: Epoch 800 Loss 0.023\n",
      "adam: Epoch 900 Loss 0.024\n",
      "adam: Epoch 1000 Loss 0.022\n",
      "Training took 158.43165063858032 seconds.\n",
      "Train accuracy in percent correct: 98.97\n",
      "Test accuracy in percent correct: 83.11\n",
      "Test accuracy in percent correct: 83.11\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results8 = []\n",
    "n_epochs = [10,20,50,70,100,500,1000]\n",
    "batch_size = 500\n",
    "learning_rate = 0.01\n",
    "n_hiddens_per_conv_layer = [5]\n",
    "patch_size_per_conv_layer = [2]\n",
    "stride_per_conv_layer = [2]\n",
    "n_hiddens_per_fc_layer = [100]\n",
    "\n",
    "for ep in n_epochs:\n",
    "    \n",
    "    print('Epochs:', ep)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    cnnet = CNN2D(16 * 16, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, len(np.unique(Ttrain)), \n",
    "              patch_size_per_conv_layer, stride_per_conv_layer, device=device)\n",
    "    \n",
    "    cnnet.train(Xtrain, Ttrain, batch_size, ep, learning_rate, method='adam')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    train_perc_correct,test_perc_correct,val_perc_correct=printstuff(Xtrain,Ttrain,Xtest,Ttest,Xval,Tval)\n",
    "    \n",
    "    experiment = [ep,batch_size,learning_rate,n_hiddens_per_conv_layer, patch_size_per_conv_layer, stride_per_conv_layer, n_hiddens_per_fc_layer, train_perc_correct, test_perc_correct, val_perc_correct, elapsed_time]\n",
    "    results8.append(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batch size</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Hiddens per convolutional layer</th>\n",
       "      <th>Patch size per convolutional layer</th>\n",
       "      <th>Stride per convolutional layer</th>\n",
       "      <th>Hiddens per fully-connected layer</th>\n",
       "      <th>Train percent correct</th>\n",
       "      <th>Test percent correct</th>\n",
       "      <th>Validate percent correct</th>\n",
       "      <th>Training time (seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>85.253545</td>\n",
       "      <td>83.106162</td>\n",
       "      <td>82.751008</td>\n",
       "      <td>1.042620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>98.965385</td>\n",
       "      <td>83.106162</td>\n",
       "      <td>82.799002</td>\n",
       "      <td>158.431651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>98.923873</td>\n",
       "      <td>83.432521</td>\n",
       "      <td>83.509311</td>\n",
       "      <td>69.359119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>97.866905</td>\n",
       "      <td>83.970052</td>\n",
       "      <td>84.593972</td>\n",
       "      <td>15.161430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>95.647592</td>\n",
       "      <td>84.795546</td>\n",
       "      <td>85.006719</td>\n",
       "      <td>6.528735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>95.864734</td>\n",
       "      <td>85.553849</td>\n",
       "      <td>85.985794</td>\n",
       "      <td>7.629286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[100]</td>\n",
       "      <td>90.468131</td>\n",
       "      <td>85.793818</td>\n",
       "      <td>85.659436</td>\n",
       "      <td>1.778397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epochs  Batch size  Learning Rate Hiddens per convolutional layer  \\\n",
       "0      10         500           0.01                             [5]   \n",
       "6    1000         500           0.01                             [5]   \n",
       "5     500         500           0.01                             [5]   \n",
       "4     100         500           0.01                             [5]   \n",
       "3      70         500           0.01                             [5]   \n",
       "2      50         500           0.01                             [5]   \n",
       "1      20         500           0.01                             [5]   \n",
       "\n",
       "  Patch size per convolutional layer Stride per convolutional layer  \\\n",
       "0                                [2]                            [2]   \n",
       "6                                [2]                            [2]   \n",
       "5                                [2]                            [2]   \n",
       "4                                [2]                            [2]   \n",
       "3                                [2]                            [2]   \n",
       "2                                [2]                            [2]   \n",
       "1                                [2]                            [2]   \n",
       "\n",
       "  Hiddens per fully-connected layer  Train percent correct  \\\n",
       "0                             [100]              85.253545   \n",
       "6                             [100]              98.965385   \n",
       "5                             [100]              98.923873   \n",
       "4                             [100]              97.866905   \n",
       "3                             [100]              95.647592   \n",
       "2                             [100]              95.864734   \n",
       "1                             [100]              90.468131   \n",
       "\n",
       "   Test percent correct  Validate percent correct  Training time (seconds)  \n",
       "0             83.106162                 82.751008                 1.042620  \n",
       "6             83.106162                 82.799002               158.431651  \n",
       "5             83.432521                 83.509311                69.359119  \n",
       "4             83.970052                 84.593972                15.161430  \n",
       "3             84.795546                 85.006719                 6.528735  \n",
       "2             85.553849                 85.985794                 7.629286  \n",
       "1             85.793818                 85.659436                 1.778397  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP8 = pd.DataFrame(results8, columns=names)\n",
    "EXP8.sort_values('Test percent correct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table we can see that, as the number of epochs is increased it takes longer time to train the model. The accuracy values for all the different number of epochs tested here are very close to each other. The model trains the fastest when number of epochs is kept 10 but might require more epochs to converge at a low RMSE value to give high accuracy on test data. We can see that the accuracy value is the highest when number of epochs is kept 20 while also training in significantly lower time. So, we go forward with keeping the number of epochs as 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Now that the experiments are done let us train our model with the best parameter values which are as follows:\n",
    "\n",
    "Number of epochs: 20\n",
    "\n",
    "Batch size: 500\n",
    "\n",
    "Learning rate: 0.01\n",
    "\n",
    "Convolutional layer network architecture: [5]\n",
    "\n",
    "Patch size for convolutional layer: [2]\n",
    "\n",
    "Stride for convolutional layer: [2]\n",
    "\n",
    "Fully-connected layer network architecture: [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam: Epoch 2 Loss 1.609\n",
      "adam: Epoch 4 Loss 1.075\n",
      "adam: Epoch 6 Loss 0.858\n",
      "adam: Epoch 8 Loss 0.783\n",
      "adam: Epoch 10 Loss 0.670\n",
      "adam: Epoch 12 Loss 0.616\n",
      "adam: Epoch 14 Loss 0.564\n",
      "adam: Epoch 16 Loss 0.515\n",
      "adam: Epoch 18 Loss 0.478\n",
      "adam: Epoch 20 Loss 0.443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Hand-drawn letters')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "n_hiddens_per_conv_layer = [5]\n",
    "patch_size_per_conv_layer = [2]\n",
    "stride_per_conv_layer=[2]\n",
    "n_hiddens_per_fc_layer = [100]\n",
    "\n",
    "cnnet = CNN2D(16 * 16, n_hiddens_per_conv_layer, n_hiddens_per_fc_layer, len(np.unique(Ttrain)), \n",
    "              patch_size_per_conv_layer, stride_per_conv_layer, device=device)\n",
    "\n",
    "cnnet.train(Xtrain, Ttrain, batch_size, n_epochs, learning_rate, method='adam')\n",
    "\n",
    "plt.plot(cnnet.error_trace, label='Pytorch')\n",
    "plt.title('Hand-drawn letters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Now, let us check the accuracy for test data with this trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.94739873296217"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_classes, Y_probs = cnnet.use(Xtest)\n",
    "accuracy(Y_classes,Ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiple experiments, it was found that the highest accuracy for test data achieved was approximately 85%. So, this is our best trained model. Let us now examine the results obtained with this trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the results, let us make a table containing values or actual class, predicted class and the corresponding probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual class</th>\n",
       "      <th>Predicted class</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>0.794346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>0.993828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>0.913059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>0.675012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>0.916177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10413</th>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>0.978104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10414</th>\n",
       "      <td>z</td>\n",
       "      <td>a</td>\n",
       "      <td>0.317287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10415</th>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>0.659790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10416</th>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "      <td>0.465811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10417</th>\n",
       "      <td>z</td>\n",
       "      <td>e</td>\n",
       "      <td>0.036449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10418 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual class Predicted class  Probability\n",
       "0                a               a     0.794346\n",
       "1                a               a     0.993828\n",
       "2                a               a     0.913059\n",
       "3                a               a     0.675012\n",
       "4                a               a     0.916177\n",
       "...            ...             ...          ...\n",
       "10413            z               z     0.978104\n",
       "10414            z               a     0.317287\n",
       "10415            z               z     0.659790\n",
       "10416            z               z     0.465811\n",
       "10417            z               e     0.036449\n",
       "\n",
       "[10418 rows x 3 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(len(Ttest)):\n",
    "    emp = [Ttest[i, 0], Y_classes[i, 0], Y_probs[i, (ord(Ttest[i, 0])-97)]]\n",
    "    res.append(emp)\n",
    "res_names = ('Actual class', 'Predicted class', 'Probability')\n",
    "m = pd.DataFrame(res, columns = res_names)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table above, the first column shows the actual class values from the test set. The second column shows the predicted class values. The third column shows probability of the correct class (actual class). Since accuracy for the test set is around 85%, there might be classes that have been wrongly classified. Such wrongly classified classes will have probability close to zero. Let us now take a look at such cases. We will first sort the table in increasing order of probability values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual class</th>\n",
       "      <th>Predicted class</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>l</td>\n",
       "      <td>p</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>r</td>\n",
       "      <td>z</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9940</th>\n",
       "      <td>x</td>\n",
       "      <td>t</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9639</th>\n",
       "      <td>v</td>\n",
       "      <td>j</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8494</th>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>0.999815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "      <td>0.999820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2039</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "      <td>0.999874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "      <td>0.999890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8467</th>\n",
       "      <td>s</td>\n",
       "      <td>s</td>\n",
       "      <td>0.999894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10418 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual class Predicted class  Probability\n",
       "5100            l               p     0.000005\n",
       "2836            f               d     0.000009\n",
       "8372            r               z     0.000013\n",
       "9940            x               t     0.000014\n",
       "9639            v               j     0.000019\n",
       "...           ...             ...          ...\n",
       "8494            s               s     0.999815\n",
       "2616            e               e     0.999820\n",
       "2039            e               e     0.999874\n",
       "2221            e               e     0.999890\n",
       "8467            s               s     0.999894\n",
       "\n",
       "[10418 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.sort_values('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright. Now we can see that the first five rows show wrongly classified classes having low probability (close to 0) for the correct class as opposed to the bottom five rows showing correctly classified classes with probabilities very close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the first row. The actual class is â€˜lâ€™ while our prediction shows it is â€˜pâ€™. Let us take a look at the actual letter from the `Xtest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAADKklEQVR4nO3dUWrDMBBAwark/ldWD1CDiQn2kzLzGwyC8FhIvGjMOX+Ant+nDwAcEydEiROixAlR4oSo18nnfspdzBjj6SOc8g/BP4dfmskJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBB1tpXCQ+7eLrmyKbLCBszKTE6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVFefN+Mqw72YXJClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0TZSrnBCtcWrHDGb2NyQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6I8uJ71ArXKqxwxpWZnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUbZSNuNahX2YnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUbZSou7eLnHvSY/JCVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghyovvm/EC+z5MTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKFspUbZLMDkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQ5TqGN4wxnj4CX8TkhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpTrGN4w57z0nGscuMLkhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRr6cPwLExxqXn5pwfPglPMTkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6KGLQZoMjkhSpwQJU6IEidEiROixAlRf4vJJt87Nr7DAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image1 = Xtest[5100]\n",
    "image1 = image1.reshape(16, 16)\n",
    "plt.imshow(-image1, cmap='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. This makes sense. Due to the loop at the top part the drawn letter this â€˜lâ€™ looks very similar to â€˜pâ€™. Hence, our model incorrectly predicts this hand-drawn â€˜lâ€™ as the letter â€˜pâ€™. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the second row. The actual class is â€˜fâ€™ while our prediction shows it is â€˜dâ€™. Let us take a look at the actual letter from the `Xtest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAADMElEQVR4nO3dwUrEMBRAUSPz/78c91qpFia9bc9ZOhTiwOWB9Jkx5/wAej7PPgCwTZwQJU6IEidEiROiXjuf+1MuvN/Y+qHJCVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQtbeVAr8aY3OZ4i2e+L+uTE6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4Iep19gFoGGOcfQS+MTkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6JspXDYnPPQczZg/sbkhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpTrGFju6DUOT2NyQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidE2Uq5mTHGoedsivSYnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTohyHcMCR69I4NlMTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEqEdupdgS+an+ncw5zz7CciYnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSoR774ftTKl69Xv4h+59/tqkxOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQoWyk3c4VrC65wxgKTE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghylbKP7jjg5VMTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUY988d11AFyByQlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcELW3leL+ATiJyQlR4oQocUKUOCFKnBAlToj6AnMFI+Y/2GKKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image2 = Xtest[2836]\n",
    "image2 = image2.reshape(16, 16)\n",
    "plt.imshow(-image2, cmap='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks very similar to the letter â€˜dâ€™. It looks more similar to â€˜dâ€™ than to â€˜fâ€™. If the labels were not mentioned, it would be hard to classify this hand-drawn letter as â€˜fâ€™. For what it looks like, our model has correctly tried to predict the letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the third row. The actual class is â€˜râ€™ while our prediction shows it is â€˜zâ€™. Let us take a look at the actual letter from the `Xtest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAADQElEQVR4nO3dUUrEQBBAwRnx/leOB9hgZHCzL0nVpyKK8miQaXpu2zaAnq9P/wDAPnFClDghSpwQJU6I+j74vH/lwvvNvQ+anBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTog6OscA/27O3esDv3rikWeTE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDgh6pFbKStbEVdx5vbGnX+PBSYnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSoRz58X3WFR+WrX/fEcwd1JidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtQ82EawqnAxVziRYAPmxe4fzeSEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTohyK+VmVjc+VrZZbJe8l8kJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCHKw/ebOfMcw+r38mD+b0xOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQoWymMMdY2Rc7cgHkikxOixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IcpWSpSND0xOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRHr7fzMpZBZpMTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKFspjDGcfygyOSFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRHn4zjKnH97L5IQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiLKVcjPOKtyHyQlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcEGUrJcodEkxOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRB2dY5in/BTAC5MTosQJUeKEKHFClDghSpwQ9QOh+i3nVR98cAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image3 = Xtest[8372]\n",
    "image3 = image3.reshape(16, 16)\n",
    "plt.imshow(-image3, cmap='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite hard to understand why our model predicted this hand-drawn â€˜râ€™ as â€˜zâ€™. Maybe the top part resembles a little like â€˜zâ€™ but still it is hard to understand this prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the fourth row. The actual class is â€˜xâ€™ while our prediction shows it is â€˜tâ€™. Let us take a look at the actual letter from the `Xtest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAADN0lEQVR4nO3dQWrDQBAAwWzQ/7+sfEBEsOBx26o62gTp0gyEHe86z/MH6Pl99wsA18QJUeKEKHFClDgh6rj53r9y4fXW1YcmJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LurmPgIda6vBHgXy5efi2TE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSo490v8A5rrdHnnec5+jy+g8kJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBD1yK2UaTtbMDZZMDkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6IeuZUyvfGxs5UyfZ8LPSYnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSoRx58nzZ50N6B+e9hckKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRNlKYdvuBsz0dRifyuSEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBDl4HvU9LUKO4fRd99x5++eeFje5IQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiLKVMmByw2Rye2P3WdMbN5/K5IQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiFo3mwXPu6AC5l2u6ZicECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiDpuvr/8mXjg9UxOiBInRIkTosQJUeKEKHFC1B9qHSfiS2CH5AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image4 = Xtest[9940]\n",
    "image4 = image4.reshape(16, 16)\n",
    "plt.imshow(-image4, cmap='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is understandable why this prediction must be wrong. The vertical line curves at the very end similar to â€˜tâ€™. The horizontal line of â€˜tâ€™ can be seen on the left part. The presence of the long vertical line in the middle might be ruining the prediction for â€˜xâ€™. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the fifth row. The actual class is â€˜vâ€™ while our prediction shows it is â€˜jâ€™. Let us take a look at the actual letter from the `Xtest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAADNElEQVR4nO3dQUrEQBBA0bTM/a8cVy6EQJyGJH/s95bKQBQ+BZKyxr7vG9Dz9fQDAMfECVHihChxQpQ4Iep18n1/yoXrjaMvmpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVFnWyl8mDEOFxxO+V9SPSYnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFC1OvpB6BhjPH2Z/Z9v+BJ+GFyQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidE2Ur5Z2Y3RWa2UriWyQlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQo5xiYNnvCYfZkxGpMTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKFspbNs2tykyu5XC35icECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROilnzx3RkBPoHJCVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQteRWCs+a2QpacSPI5IQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcEOXF9zc44/Db7M81+3tcjckJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBC15FbK3dsUzg8ww+SEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTohacitl1p3bLO6JYHJClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTojy4vsNnFZghskJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBB1tpXiJgA8xOSEKHFClDghSpwQJU6IEidEfQOcxCzmVfHoQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image5 = Xtest[9639]\n",
    "image5 = image5.reshape(16, 16)\n",
    "plt.imshow(-image5, cmap='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks very similar to the letter â€˜jâ€™ and hardly looks anything like â€˜vâ€™. The curves of â€˜jâ€™ have been accurately represented in this hand-drawn â€˜vâ€™. Hence, our model predicts this as â€˜jâ€™ and not â€˜vâ€™."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some of the wrongly predicted classes by our trained model. The reasonings for wrong predictions are also somewhat acceptable. Most of the time the letters drawn do not resemble like their actual class but a look a lot like a different class which are in fact correctly predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand these predictions, let us use confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "conf = confusion_matrix(Y_classes, Ttest)\n",
    "colorful_conf = conf.style.background_gradient(cmap='Blues').format(\"{:.1f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col25{\n",
       "            background-color:  #08306b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col22{\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col23{\n",
       "            background-color:  #f6faff;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col2,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col18{\n",
       "            background-color:  #f5f9fe;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col5{\n",
       "            background-color:  #f1f7fd;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row4_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row14_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col25,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col22,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col12,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col13{\n",
       "            background-color:  #f5fafe;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col13{\n",
       "            background-color:  #edf4fc;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row0_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col9,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col3,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col24{\n",
       "            background-color:  #f3f8fe;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row2_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col15,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col23,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row17_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col21,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col19{\n",
       "            background-color:  #f4f9fe;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col7,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col6{\n",
       "            background-color:  #f2f7fd;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row1_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col4,#T_08861d95_b068_11eb_8f39_c1c58594a992row13_col20,#T_08861d95_b068_11eb_8f39_c1c58594a992row15_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col24,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col13{\n",
       "            background-color:  #f2f8fd;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col0,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col15{\n",
       "            background-color:  #eaf3fb;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col1{\n",
       "            background-color:  #eff6fc;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row3_col14,#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col1,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col18{\n",
       "            background-color:  #eef5fc;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col10{\n",
       "            background-color:  #e6f0f9;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col17,#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row19_col5,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col13,#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col19{\n",
       "            background-color:  #f0f6fd;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row5_col19,#T_08861d95_b068_11eb_8f39_c1c58594a992row18_col6{\n",
       "            background-color:  #e9f2fa;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row6_col16,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col6,#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col11,#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col17{\n",
       "            background-color:  #eaf2fb;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col10,#T_08861d95_b068_11eb_8f39_c1c58594a992row10_col7{\n",
       "            background-color:  #ecf4fb;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row7_col13{\n",
       "            background-color:  #e7f1fa;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row8_col11{\n",
       "            background-color:  #e1edf8;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col8,#T_08861d95_b068_11eb_8f39_c1c58594a992row20_col21{\n",
       "            background-color:  #ebf3fb;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row9_col18,#T_08861d95_b068_11eb_8f39_c1c58594a992row22_col20{\n",
       "            background-color:  #e5eff9;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row11_col8{\n",
       "            background-color:  #d5e5f4;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row12_col13{\n",
       "            background-color:  #e7f0fa;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row16_col6{\n",
       "            background-color:  #c8dcf0;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row21_col20{\n",
       "            background-color:  #cddff1;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row23_col24{\n",
       "            background-color:  #e8f1fa;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row24_col6{\n",
       "            background-color:  #b3d3e8;\n",
       "            color:  #000000;\n",
       "        }#T_08861d95_b068_11eb_8f39_c1c58594a992row25_col4{\n",
       "            background-color:  #cfe1f2;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_08861d95_b068_11eb_8f39_c1c58594a992\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >a</th>        <th class=\"col_heading level0 col1\" >b</th>        <th class=\"col_heading level0 col2\" >c</th>        <th class=\"col_heading level0 col3\" >d</th>        <th class=\"col_heading level0 col4\" >e</th>        <th class=\"col_heading level0 col5\" >f</th>        <th class=\"col_heading level0 col6\" >g</th>        <th class=\"col_heading level0 col7\" >h</th>        <th class=\"col_heading level0 col8\" >i</th>        <th class=\"col_heading level0 col9\" >j</th>        <th class=\"col_heading level0 col10\" >k</th>        <th class=\"col_heading level0 col11\" >l</th>        <th class=\"col_heading level0 col12\" >m</th>        <th class=\"col_heading level0 col13\" >n</th>        <th class=\"col_heading level0 col14\" >o</th>        <th class=\"col_heading level0 col15\" >p</th>        <th class=\"col_heading level0 col16\" >q</th>        <th class=\"col_heading level0 col17\" >r</th>        <th class=\"col_heading level0 col18\" >s</th>        <th class=\"col_heading level0 col19\" >t</th>        <th class=\"col_heading level0 col20\" >u</th>        <th class=\"col_heading level0 col21\" >v</th>        <th class=\"col_heading level0 col22\" >w</th>        <th class=\"col_heading level0 col23\" >x</th>        <th class=\"col_heading level0 col24\" >y</th>        <th class=\"col_heading level0 col25\" >z</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row0\" class=\"row_heading level0 row0\" >a</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col0\" class=\"data row0 col0\" >82.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col1\" class=\"data row0 col1\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col2\" class=\"data row0 col2\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col3\" class=\"data row0 col3\" >1.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col4\" class=\"data row0 col4\" >3.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col5\" class=\"data row0 col5\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col6\" class=\"data row0 col6\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col7\" class=\"data row0 col7\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col8\" class=\"data row0 col8\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col9\" class=\"data row0 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col10\" class=\"data row0 col10\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col11\" class=\"data row0 col11\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col12\" class=\"data row0 col12\" >0.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col13\" class=\"data row0 col13\" >5.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col14\" class=\"data row0 col14\" >2.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col15\" class=\"data row0 col15\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col16\" class=\"data row0 col16\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col17\" class=\"data row0 col17\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col18\" class=\"data row0 col18\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col19\" class=\"data row0 col19\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col20\" class=\"data row0 col20\" >1.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col21\" class=\"data row0 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col22\" class=\"data row0 col22\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col23\" class=\"data row0 col23\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col24\" class=\"data row0 col24\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row0_col25\" class=\"data row0 col25\" >0.1 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row1\" class=\"row_heading level0 row1\" >b</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col0\" class=\"data row1 col0\" >1.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col1\" class=\"data row1 col1\" >88.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col2\" class=\"data row1 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col3\" class=\"data row1 col3\" >0.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col4\" class=\"data row1 col4\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col5\" class=\"data row1 col5\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col6\" class=\"data row1 col6\" >1.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col7\" class=\"data row1 col7\" >2.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col8\" class=\"data row1 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col9\" class=\"data row1 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col10\" class=\"data row1 col10\" >2.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col11\" class=\"data row1 col11\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col12\" class=\"data row1 col12\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col13\" class=\"data row1 col13\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col14\" class=\"data row1 col14\" >0.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col15\" class=\"data row1 col15\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col16\" class=\"data row1 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col17\" class=\"data row1 col17\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col18\" class=\"data row1 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col19\" class=\"data row1 col19\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col20\" class=\"data row1 col20\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col21\" class=\"data row1 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col22\" class=\"data row1 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col23\" class=\"data row1 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col24\" class=\"data row1 col24\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row1_col25\" class=\"data row1 col25\" >0.8 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row2\" class=\"row_heading level0 row2\" >c</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col0\" class=\"data row2 col0\" >1.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col1\" class=\"data row2 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col2\" class=\"data row2 col2\" >91.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col3\" class=\"data row2 col3\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col4\" class=\"data row2 col4\" >3.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col5\" class=\"data row2 col5\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col6\" class=\"data row2 col6\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col7\" class=\"data row2 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col8\" class=\"data row2 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col9\" class=\"data row2 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col10\" class=\"data row2 col10\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col11\" class=\"data row2 col11\" >0.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col12\" class=\"data row2 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col13\" class=\"data row2 col13\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col14\" class=\"data row2 col14\" >1.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col15\" class=\"data row2 col15\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col16\" class=\"data row2 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col17\" class=\"data row2 col17\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col18\" class=\"data row2 col18\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col19\" class=\"data row2 col19\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col20\" class=\"data row2 col20\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col21\" class=\"data row2 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col22\" class=\"data row2 col22\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col23\" class=\"data row2 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col24\" class=\"data row2 col24\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row2_col25\" class=\"data row2 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row3\" class=\"row_heading level0 row3\" >d</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col0\" class=\"data row3 col0\" >5.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col1\" class=\"data row3 col1\" >3.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col2\" class=\"data row3 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col3\" class=\"data row3 col3\" >79.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col4\" class=\"data row3 col4\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col5\" class=\"data row3 col5\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col6\" class=\"data row3 col6\" >1.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col7\" class=\"data row3 col7\" >1.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col8\" class=\"data row3 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col9\" class=\"data row3 col9\" >1.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col10\" class=\"data row3 col10\" >1.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col11\" class=\"data row3 col11\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col12\" class=\"data row3 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col13\" class=\"data row3 col13\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col14\" class=\"data row3 col14\" >4.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col15\" class=\"data row3 col15\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col16\" class=\"data row3 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col17\" class=\"data row3 col17\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col18\" class=\"data row3 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col19\" class=\"data row3 col19\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col20\" class=\"data row3 col20\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col21\" class=\"data row3 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col22\" class=\"data row3 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col23\" class=\"data row3 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col24\" class=\"data row3 col24\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row3_col25\" class=\"data row3 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row4\" class=\"row_heading level0 row4\" >e</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col0\" class=\"data row4 col0\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col1\" class=\"data row4 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col2\" class=\"data row4 col2\" >1.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col3\" class=\"data row4 col3\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col4\" class=\"data row4 col4\" >93.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col5\" class=\"data row4 col5\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col6\" class=\"data row4 col6\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col7\" class=\"data row4 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col8\" class=\"data row4 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col9\" class=\"data row4 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col10\" class=\"data row4 col10\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col11\" class=\"data row4 col11\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col12\" class=\"data row4 col12\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col13\" class=\"data row4 col13\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col14\" class=\"data row4 col14\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col15\" class=\"data row4 col15\" >1.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col16\" class=\"data row4 col16\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col17\" class=\"data row4 col17\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col18\" class=\"data row4 col18\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col19\" class=\"data row4 col19\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col20\" class=\"data row4 col20\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col21\" class=\"data row4 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col22\" class=\"data row4 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col23\" class=\"data row4 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col24\" class=\"data row4 col24\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row4_col25\" class=\"data row4 col25\" >0.3 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row5\" class=\"row_heading level0 row5\" >f</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col0\" class=\"data row5 col0\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col1\" class=\"data row5 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col2\" class=\"data row5 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col3\" class=\"data row5 col3\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col4\" class=\"data row5 col4\" >1.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col5\" class=\"data row5 col5\" >77.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col6\" class=\"data row5 col6\" >1.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col7\" class=\"data row5 col7\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col8\" class=\"data row5 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col9\" class=\"data row5 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col10\" class=\"data row5 col10\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col11\" class=\"data row5 col11\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col12\" class=\"data row5 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col13\" class=\"data row5 col13\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col14\" class=\"data row5 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col15\" class=\"data row5 col15\" >1.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col16\" class=\"data row5 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col17\" class=\"data row5 col17\" >7.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col18\" class=\"data row5 col18\" >3.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col19\" class=\"data row5 col19\" >6.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col20\" class=\"data row5 col20\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col21\" class=\"data row5 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col22\" class=\"data row5 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col23\" class=\"data row5 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col24\" class=\"data row5 col24\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row5_col25\" class=\"data row5 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row6\" class=\"row_heading level0 row6\" >g</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col0\" class=\"data row6 col0\" >2.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col1\" class=\"data row6 col1\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col2\" class=\"data row6 col2\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col3\" class=\"data row6 col3\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col4\" class=\"data row6 col4\" >1.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col5\" class=\"data row6 col5\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col6\" class=\"data row6 col6\" >79.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col7\" class=\"data row6 col7\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col8\" class=\"data row6 col8\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col9\" class=\"data row6 col9\" >1.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col10\" class=\"data row6 col10\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col11\" class=\"data row6 col11\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col12\" class=\"data row6 col12\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col13\" class=\"data row6 col13\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col14\" class=\"data row6 col14\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col15\" class=\"data row6 col15\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col16\" class=\"data row6 col16\" >3.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col17\" class=\"data row6 col17\" >1.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col18\" class=\"data row6 col18\" >4.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col19\" class=\"data row6 col19\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col20\" class=\"data row6 col20\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col21\" class=\"data row6 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col22\" class=\"data row6 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col23\" class=\"data row6 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col24\" class=\"data row6 col24\" >1.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row6_col25\" class=\"data row6 col25\" >0.2 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row7\" class=\"row_heading level0 row7\" >h</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col0\" class=\"data row7 col0\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col1\" class=\"data row7 col1\" >3.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col2\" class=\"data row7 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col3\" class=\"data row7 col3\" >1.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col4\" class=\"data row7 col4\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col5\" class=\"data row7 col5\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col6\" class=\"data row7 col6\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col7\" class=\"data row7 col7\" >79.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col8\" class=\"data row7 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col9\" class=\"data row7 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col10\" class=\"data row7 col10\" >4.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col11\" class=\"data row7 col11\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col12\" class=\"data row7 col12\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col13\" class=\"data row7 col13\" >7.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col14\" class=\"data row7 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col15\" class=\"data row7 col15\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col16\" class=\"data row7 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col17\" class=\"data row7 col17\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col18\" class=\"data row7 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col19\" class=\"data row7 col19\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col20\" class=\"data row7 col20\" >1.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col21\" class=\"data row7 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col22\" class=\"data row7 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col23\" class=\"data row7 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col24\" class=\"data row7 col24\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row7_col25\" class=\"data row7 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row8\" class=\"row_heading level0 row8\" >i</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col0\" class=\"data row8 col0\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col1\" class=\"data row8 col1\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col2\" class=\"data row8 col2\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col3\" class=\"data row8 col3\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col4\" class=\"data row8 col4\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col5\" class=\"data row8 col5\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col6\" class=\"data row8 col6\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col7\" class=\"data row8 col7\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col8\" class=\"data row8 col8\" >87.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col9\" class=\"data row8 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col10\" class=\"data row8 col10\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col11\" class=\"data row8 col11\" >9.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col12\" class=\"data row8 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col13\" class=\"data row8 col13\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col14\" class=\"data row8 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col15\" class=\"data row8 col15\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col16\" class=\"data row8 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col17\" class=\"data row8 col17\" >0.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col18\" class=\"data row8 col18\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col19\" class=\"data row8 col19\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col20\" class=\"data row8 col20\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col21\" class=\"data row8 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col22\" class=\"data row8 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col23\" class=\"data row8 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col24\" class=\"data row8 col24\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row8_col25\" class=\"data row8 col25\" >0.2 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row9\" class=\"row_heading level0 row9\" >j</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col0\" class=\"data row9 col0\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col1\" class=\"data row9 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col2\" class=\"data row9 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col3\" class=\"data row9 col3\" >2.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col4\" class=\"data row9 col4\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col5\" class=\"data row9 col5\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col6\" class=\"data row9 col6\" >5.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col7\" class=\"data row9 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col8\" class=\"data row9 col8\" >5.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col9\" class=\"data row9 col9\" >70.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col10\" class=\"data row9 col10\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col11\" class=\"data row9 col11\" >5.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col12\" class=\"data row9 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col13\" class=\"data row9 col13\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col14\" class=\"data row9 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col15\" class=\"data row9 col15\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col16\" class=\"data row9 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col17\" class=\"data row9 col17\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col18\" class=\"data row9 col18\" >8.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col19\" class=\"data row9 col19\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col20\" class=\"data row9 col20\" >2.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col21\" class=\"data row9 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col22\" class=\"data row9 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col23\" class=\"data row9 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col24\" class=\"data row9 col24\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row9_col25\" class=\"data row9 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row10\" class=\"row_heading level0 row10\" >k</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col0\" class=\"data row10 col0\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col1\" class=\"data row10 col1\" >2.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col2\" class=\"data row10 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col3\" class=\"data row10 col3\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col4\" class=\"data row10 col4\" >2.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col5\" class=\"data row10 col5\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col6\" class=\"data row10 col6\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col7\" class=\"data row10 col7\" >4.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col8\" class=\"data row10 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col9\" class=\"data row10 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col10\" class=\"data row10 col10\" >82.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col11\" class=\"data row10 col11\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col12\" class=\"data row10 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col13\" class=\"data row10 col13\" >1.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col14\" class=\"data row10 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col15\" class=\"data row10 col15\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col16\" class=\"data row10 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col17\" class=\"data row10 col17\" >3.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col18\" class=\"data row10 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col19\" class=\"data row10 col19\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col20\" class=\"data row10 col20\" >1.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col21\" class=\"data row10 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col22\" class=\"data row10 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col23\" class=\"data row10 col23\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col24\" class=\"data row10 col24\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row10_col25\" class=\"data row10 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row11\" class=\"row_heading level0 row11\" >l</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col0\" class=\"data row11 col0\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col1\" class=\"data row11 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col2\" class=\"data row11 col2\" >1.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col3\" class=\"data row11 col3\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col4\" class=\"data row11 col4\" >0.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col5\" class=\"data row11 col5\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col6\" class=\"data row11 col6\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col7\" class=\"data row11 col7\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col8\" class=\"data row11 col8\" >15.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col9\" class=\"data row11 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col10\" class=\"data row11 col10\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col11\" class=\"data row11 col11\" >81.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col12\" class=\"data row11 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col13\" class=\"data row11 col13\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col14\" class=\"data row11 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col15\" class=\"data row11 col15\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col16\" class=\"data row11 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col17\" class=\"data row11 col17\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col18\" class=\"data row11 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col19\" class=\"data row11 col19\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col20\" class=\"data row11 col20\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col21\" class=\"data row11 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col22\" class=\"data row11 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col23\" class=\"data row11 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col24\" class=\"data row11 col24\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row11_col25\" class=\"data row11 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row12\" class=\"row_heading level0 row12\" >m</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col0\" class=\"data row12 col0\" >1.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col1\" class=\"data row12 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col2\" class=\"data row12 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col3\" class=\"data row12 col3\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col4\" class=\"data row12 col4\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col5\" class=\"data row12 col5\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col6\" class=\"data row12 col6\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col7\" class=\"data row12 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col8\" class=\"data row12 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col9\" class=\"data row12 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col10\" class=\"data row12 col10\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col11\" class=\"data row12 col11\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col12\" class=\"data row12 col12\" >88.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col13\" class=\"data row12 col13\" >7.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col14\" class=\"data row12 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col15\" class=\"data row12 col15\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col16\" class=\"data row12 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col17\" class=\"data row12 col17\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col18\" class=\"data row12 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col19\" class=\"data row12 col19\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col20\" class=\"data row12 col20\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col21\" class=\"data row12 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col22\" class=\"data row12 col22\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col23\" class=\"data row12 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col24\" class=\"data row12 col24\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row12_col25\" class=\"data row12 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row13\" class=\"row_heading level0 row13\" >n</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col0\" class=\"data row13 col0\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col1\" class=\"data row13 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col2\" class=\"data row13 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col3\" class=\"data row13 col3\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col4\" class=\"data row13 col4\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col5\" class=\"data row13 col5\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col6\" class=\"data row13 col6\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col7\" class=\"data row13 col7\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col8\" class=\"data row13 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col9\" class=\"data row13 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col10\" class=\"data row13 col10\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col11\" class=\"data row13 col11\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col12\" class=\"data row13 col12\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col13\" class=\"data row13 col13\" >94.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col14\" class=\"data row13 col14\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col15\" class=\"data row13 col15\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col16\" class=\"data row13 col16\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col17\" class=\"data row13 col17\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col18\" class=\"data row13 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col19\" class=\"data row13 col19\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col20\" class=\"data row13 col20\" >2.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col21\" class=\"data row13 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col22\" class=\"data row13 col22\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col23\" class=\"data row13 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col24\" class=\"data row13 col24\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row13_col25\" class=\"data row13 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row14\" class=\"row_heading level0 row14\" >o</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col0\" class=\"data row14 col0\" >1.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col1\" class=\"data row14 col1\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col2\" class=\"data row14 col2\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col3\" class=\"data row14 col3\" >0.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col4\" class=\"data row14 col4\" >0.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col5\" class=\"data row14 col5\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col6\" class=\"data row14 col6\" >0.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col7\" class=\"data row14 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col8\" class=\"data row14 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col9\" class=\"data row14 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col10\" class=\"data row14 col10\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col11\" class=\"data row14 col11\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col12\" class=\"data row14 col12\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col13\" class=\"data row14 col13\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col14\" class=\"data row14 col14\" >95.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col15\" class=\"data row14 col15\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col16\" class=\"data row14 col16\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col17\" class=\"data row14 col17\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col18\" class=\"data row14 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col19\" class=\"data row14 col19\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col20\" class=\"data row14 col20\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col21\" class=\"data row14 col21\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col22\" class=\"data row14 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col23\" class=\"data row14 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col24\" class=\"data row14 col24\" >0.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row14_col25\" class=\"data row14 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row15\" class=\"row_heading level0 row15\" >p</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col0\" class=\"data row15 col0\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col1\" class=\"data row15 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col2\" class=\"data row15 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col3\" class=\"data row15 col3\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col4\" class=\"data row15 col4\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col5\" class=\"data row15 col5\" >1.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col6\" class=\"data row15 col6\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col7\" class=\"data row15 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col8\" class=\"data row15 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col9\" class=\"data row15 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col10\" class=\"data row15 col10\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col11\" class=\"data row15 col11\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col12\" class=\"data row15 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col13\" class=\"data row15 col13\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col14\" class=\"data row15 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col15\" class=\"data row15 col15\" >92.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col16\" class=\"data row15 col16\" >1.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col17\" class=\"data row15 col17\" >2.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col18\" class=\"data row15 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col19\" class=\"data row15 col19\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col20\" class=\"data row15 col20\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col21\" class=\"data row15 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col22\" class=\"data row15 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col23\" class=\"data row15 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col24\" class=\"data row15 col24\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row15_col25\" class=\"data row15 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row16\" class=\"row_heading level0 row16\" >q</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col0\" class=\"data row16 col0\" >4.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col1\" class=\"data row16 col1\" >4.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col2\" class=\"data row16 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col3\" class=\"data row16 col3\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col4\" class=\"data row16 col4\" >2.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col5\" class=\"data row16 col5\" >2.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col6\" class=\"data row16 col6\" >19.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col7\" class=\"data row16 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col8\" class=\"data row16 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col9\" class=\"data row16 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col10\" class=\"data row16 col10\" >1.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col11\" class=\"data row16 col11\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col12\" class=\"data row16 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col13\" class=\"data row16 col13\" >1.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col14\" class=\"data row16 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col15\" class=\"data row16 col15\" >5.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col16\" class=\"data row16 col16\" >52.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col17\" class=\"data row16 col17\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col18\" class=\"data row16 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col19\" class=\"data row16 col19\" >1.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col20\" class=\"data row16 col20\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col21\" class=\"data row16 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col22\" class=\"data row16 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col23\" class=\"data row16 col23\" >1.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col24\" class=\"data row16 col24\" >1.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row16_col25\" class=\"data row16 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row17\" class=\"row_heading level0 row17\" >r</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col0\" class=\"data row17 col0\" >1.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col1\" class=\"data row17 col1\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col2\" class=\"data row17 col2\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col3\" class=\"data row17 col3\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col4\" class=\"data row17 col4\" >1.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col5\" class=\"data row17 col5\" >1.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col6\" class=\"data row17 col6\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col7\" class=\"data row17 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col8\" class=\"data row17 col8\" >1.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col9\" class=\"data row17 col9\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col10\" class=\"data row17 col10\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col11\" class=\"data row17 col11\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col12\" class=\"data row17 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col13\" class=\"data row17 col13\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col14\" class=\"data row17 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col15\" class=\"data row17 col15\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col16\" class=\"data row17 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col17\" class=\"data row17 col17\" >86.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col18\" class=\"data row17 col18\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col19\" class=\"data row17 col19\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col20\" class=\"data row17 col20\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col21\" class=\"data row17 col21\" >1.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col22\" class=\"data row17 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col23\" class=\"data row17 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col24\" class=\"data row17 col24\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row17_col25\" class=\"data row17 col25\" >0.4 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row18\" class=\"row_heading level0 row18\" >s</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col0\" class=\"data row18 col0\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col1\" class=\"data row18 col1\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col2\" class=\"data row18 col2\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col3\" class=\"data row18 col3\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col4\" class=\"data row18 col4\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col5\" class=\"data row18 col5\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col6\" class=\"data row18 col6\" >5.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col7\" class=\"data row18 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col8\" class=\"data row18 col8\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col9\" class=\"data row18 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col10\" class=\"data row18 col10\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col11\" class=\"data row18 col11\" >1.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col12\" class=\"data row18 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col13\" class=\"data row18 col13\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col14\" class=\"data row18 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col15\" class=\"data row18 col15\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col16\" class=\"data row18 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col17\" class=\"data row18 col17\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col18\" class=\"data row18 col18\" >89.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col19\" class=\"data row18 col19\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col20\" class=\"data row18 col20\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col21\" class=\"data row18 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col22\" class=\"data row18 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col23\" class=\"data row18 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col24\" class=\"data row18 col24\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row18_col25\" class=\"data row18 col25\" >0.4 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row19\" class=\"row_heading level0 row19\" >t</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col0\" class=\"data row19 col0\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col1\" class=\"data row19 col1\" >0.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col2\" class=\"data row19 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col3\" class=\"data row19 col3\" >0.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col4\" class=\"data row19 col4\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col5\" class=\"data row19 col5\" >2.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col6\" class=\"data row19 col6\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col7\" class=\"data row19 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col8\" class=\"data row19 col8\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col9\" class=\"data row19 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col10\" class=\"data row19 col10\" >0.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col11\" class=\"data row19 col11\" >0.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col12\" class=\"data row19 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col13\" class=\"data row19 col13\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col14\" class=\"data row19 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col15\" class=\"data row19 col15\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col16\" class=\"data row19 col16\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col17\" class=\"data row19 col17\" >4.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col18\" class=\"data row19 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col19\" class=\"data row19 col19\" >84.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col20\" class=\"data row19 col20\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col21\" class=\"data row19 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col22\" class=\"data row19 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col23\" class=\"data row19 col23\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col24\" class=\"data row19 col24\" >1.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row19_col25\" class=\"data row19 col25\" >0.7 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row20\" class=\"row_heading level0 row20\" >u</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col0\" class=\"data row20 col0\" >4.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col1\" class=\"data row20 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col2\" class=\"data row20 col2\" >0.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col3\" class=\"data row20 col3\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col4\" class=\"data row20 col4\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col5\" class=\"data row20 col5\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col6\" class=\"data row20 col6\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col7\" class=\"data row20 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col8\" class=\"data row20 col8\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col9\" class=\"data row20 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col10\" class=\"data row20 col10\" >1.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col11\" class=\"data row20 col11\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col12\" class=\"data row20 col12\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col13\" class=\"data row20 col13\" >3.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col14\" class=\"data row20 col14\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col15\" class=\"data row20 col15\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col16\" class=\"data row20 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col17\" class=\"data row20 col17\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col18\" class=\"data row20 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col19\" class=\"data row20 col19\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col20\" class=\"data row20 col20\" >83.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col21\" class=\"data row20 col21\" >4.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col22\" class=\"data row20 col22\" >1.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col23\" class=\"data row20 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col24\" class=\"data row20 col24\" >0.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row20_col25\" class=\"data row20 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row21\" class=\"row_heading level0 row21\" >v</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col0\" class=\"data row21 col0\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col1\" class=\"data row21 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col2\" class=\"data row21 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col3\" class=\"data row21 col3\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col4\" class=\"data row21 col4\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col5\" class=\"data row21 col5\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col6\" class=\"data row21 col6\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col7\" class=\"data row21 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col8\" class=\"data row21 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col9\" class=\"data row21 col9\" >0.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col10\" class=\"data row21 col10\" >0.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col11\" class=\"data row21 col11\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col12\" class=\"data row21 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col13\" class=\"data row21 col13\" >0.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col14\" class=\"data row21 col14\" >0.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col15\" class=\"data row21 col15\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col16\" class=\"data row21 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col17\" class=\"data row21 col17\" >6.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col18\" class=\"data row21 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col19\" class=\"data row21 col19\" >1.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col20\" class=\"data row21 col20\" >18.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col21\" class=\"data row21 col21\" >70.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col22\" class=\"data row21 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col23\" class=\"data row21 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col24\" class=\"data row21 col24\" >0.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row21_col25\" class=\"data row21 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row22\" class=\"row_heading level0 row22\" >w</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col0\" class=\"data row22 col0\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col1\" class=\"data row22 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col2\" class=\"data row22 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col3\" class=\"data row22 col3\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col4\" class=\"data row22 col4\" >1.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col5\" class=\"data row22 col5\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col6\" class=\"data row22 col6\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col7\" class=\"data row22 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col8\" class=\"data row22 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col9\" class=\"data row22 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col10\" class=\"data row22 col10\" >1.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col11\" class=\"data row22 col11\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col12\" class=\"data row22 col12\" >1.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col13\" class=\"data row22 col13\" >4.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col14\" class=\"data row22 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col15\" class=\"data row22 col15\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col16\" class=\"data row22 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col17\" class=\"data row22 col17\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col18\" class=\"data row22 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col19\" class=\"data row22 col19\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col20\" class=\"data row22 col20\" >7.7 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col21\" class=\"data row22 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col22\" class=\"data row22 col22\" >84.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col23\" class=\"data row22 col23\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col24\" class=\"data row22 col24\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row22_col25\" class=\"data row22 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row23\" class=\"row_heading level0 row23\" >x</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col0\" class=\"data row23 col0\" >1.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col1\" class=\"data row23 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col2\" class=\"data row23 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col3\" class=\"data row23 col3\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col4\" class=\"data row23 col4\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col5\" class=\"data row23 col5\" >2.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col6\" class=\"data row23 col6\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col7\" class=\"data row23 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col8\" class=\"data row23 col8\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col9\" class=\"data row23 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col10\" class=\"data row23 col10\" >7.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col11\" class=\"data row23 col11\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col12\" class=\"data row23 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col13\" class=\"data row23 col13\" >2.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col14\" class=\"data row23 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col15\" class=\"data row23 col15\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col16\" class=\"data row23 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col17\" class=\"data row23 col17\" >2.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col18\" class=\"data row23 col18\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col19\" class=\"data row23 col19\" >1.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col20\" class=\"data row23 col20\" >1.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col21\" class=\"data row23 col21\" >1.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col22\" class=\"data row23 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col23\" class=\"data row23 col23\" >75.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col24\" class=\"data row23 col24\" >4.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row23_col25\" class=\"data row23 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row24\" class=\"row_heading level0 row24\" >y</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col0\" class=\"data row24 col0\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col1\" class=\"data row24 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col2\" class=\"data row24 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col3\" class=\"data row24 col3\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col4\" class=\"data row24 col4\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col5\" class=\"data row24 col5\" >1.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col6\" class=\"data row24 col6\" >24.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col7\" class=\"data row24 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col8\" class=\"data row24 col8\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col9\" class=\"data row24 col9\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col10\" class=\"data row24 col10\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col11\" class=\"data row24 col11\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col12\" class=\"data row24 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col13\" class=\"data row24 col13\" >0.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col14\" class=\"data row24 col14\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col15\" class=\"data row24 col15\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col16\" class=\"data row24 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col17\" class=\"data row24 col17\" >2.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col18\" class=\"data row24 col18\" >1.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col19\" class=\"data row24 col19\" >1.6 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col20\" class=\"data row24 col20\" >0.8 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col21\" class=\"data row24 col21\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col22\" class=\"data row24 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col23\" class=\"data row24 col23\" >0.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col24\" class=\"data row24 col24\" >64.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row24_col25\" class=\"data row24 col25\" >0.0 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_08861d95_b068_11eb_8f39_c1c58594a992level0_row25\" class=\"row_heading level0 row25\" >z</th>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col0\" class=\"data row25 col0\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col1\" class=\"data row25 col1\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col2\" class=\"data row25 col2\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col3\" class=\"data row25 col3\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col4\" class=\"data row25 col4\" >19.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col5\" class=\"data row25 col5\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col6\" class=\"data row25 col6\" >2.3 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col7\" class=\"data row25 col7\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col8\" class=\"data row25 col8\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col9\" class=\"data row25 col9\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col10\" class=\"data row25 col10\" >0.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col11\" class=\"data row25 col11\" >0.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col12\" class=\"data row25 col12\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col13\" class=\"data row25 col13\" >0.9 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col14\" class=\"data row25 col14\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col15\" class=\"data row25 col15\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col16\" class=\"data row25 col16\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col17\" class=\"data row25 col17\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col18\" class=\"data row25 col18\" >4.1 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col19\" class=\"data row25 col19\" >3.2 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col20\" class=\"data row25 col20\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col21\" class=\"data row25 col21\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col22\" class=\"data row25 col22\" >0.0 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col23\" class=\"data row25 col23\" >0.5 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col24\" class=\"data row25 col24\" >1.4 %</td>\n",
       "                        <td id=\"T_08861d95_b068_11eb_8f39_c1c58594a992row25_col25\" class=\"data row25 col25\" >65.1 %</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d7d65ce040>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colorful_conf\n",
    "#For some reason the colors disappear when closing and reopening the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix we can see that our model predicts the correct class for the most part. However, there are instances where our model predicts incorrectly. These instances are very few but still interesting to look at. Like the letter â€˜iâ€™ which is very close to â€˜jâ€™ without the curved part at the bottom or the letters â€˜iâ€™ and â€˜lâ€™ which are again very similar without the dot on top. The model also confuses â€˜fâ€™ with â€˜râ€™ which are similar without the horizontal line in the middle. Letters â€˜fâ€™ and â€˜tâ€™ are also confused few times. These look very similar too without the curves at the top and at the bottom. Another set of confused letters is â€˜qâ€™ and â€˜gâ€™. Without the curve at the bottom â€˜qâ€™ looks very similar to â€˜gâ€™. â€˜gâ€™ is also confused with â€˜yâ€™ whose bottom parts are similar to each other. â€˜vâ€™ and â€˜uâ€™ are also confusing the model as they are very much alike. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the experiments were successfully performed, and the best model trained with the hyperparameters using Convolutional Neural Network algorithm gave around 85% accuracy on the test data. Training this model took around 2 seconds which is actually very fast. The falsely predicted classes were also justified with acceptable reasoning.\n",
    "\n",
    "Through this project, I learned that the prediction difficulty increases as the number of classes increase in the dataset and experimenting with multiple values for each of the hyperparameters greatly helps in finding that â€˜bestâ€™ set of parameters that will give high accuracy for prediction. One hurdle that I stumbled upon was the â€˜CUDA is out of memory errorâ€™ while running all experiments in one go. Hence, these experiments had to be categorized to run them without getting the error. Nevertheless, the experiments were performed successfully within sufficient amount of time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
